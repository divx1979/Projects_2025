{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0faae362-cb2a-47e9-bf38-c2f3bcbf0574",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Here’s a complete PySpark pipeline to analyze and identify store closure due to severe weather using the available tables (salestransskufact, salestransorderfact, weatherdata, employee, customer, etc.) for All Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50912862-eb08-4c5a-83a4-c363a7ee2751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6c3928-0920-4f05-9ce5-8e9f4c5f9eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.salestransskufact\n",
    "\"\"\"\n",
    "\n",
    "df_sales = spark.sql(query)\n",
    "display(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e833b3-051d-466c-a392-c1a81fb3986a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_df = spark.createDataFrame([(col,) for col in df_sales.columns], [\"Column Name\"])\n",
    "display(columns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a48503-eef7-4828-a91b-ac1a0d640e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.salestransorderfact\n",
    "\"\"\"\n",
    "\n",
    "df_sales_trans = spark.sql(query)\n",
    "display(df_sales_trans)\n",
    "\n",
    "print(df_sales_trans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4ad248-4bae-4a65-b6c6-00bfedab5a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Convert OrderDate to date type and rename for join\n",
    "df_sales_trans = df_sales_trans.withColumn(\"date\", to_date(col(\"OrderDate\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f5fc00-1cd7-4153-a800-6785d2c1530d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.weatherdata_daily_historical_openmeteo_nws\n",
    "\"\"\"\n",
    "\n",
    "df_wethr = spark.sql(query)\n",
    "display(df_wethr)\n",
    "\n",
    "print(df_wethr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27390392-332f-4f73-9628-6d909daad380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.weatherdata_daily_forecast_nws\n",
    "\"\"\"\n",
    "\n",
    "df_forecasted = spark.sql(query)\n",
    "display(df_forecasted)\n",
    "\n",
    "print(df_forecasted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e4d655-3366-4fcc-8a15-a8c991d20879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.calendar\n",
    "\"\"\"\n",
    "\n",
    "df_calendar = spark.sql(query)\n",
    "display(df_calendar)\n",
    "\n",
    "print(df_calendar.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d884919-74af-4fff-a650-bd7534e1b0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM gold.product\n",
    "\"\"\"\n",
    "\n",
    "df_pr = spark.sql(query)\n",
    "display(df_pr)\n",
    "\n",
    "print(df_pr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6d61c8-fca5-4e4a-8323-e8c439d96c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# We can try to join gold.weatherdata and gold.weatherdata_forecasted right at the beginning — this will give us a unified view where each row contains both the actual and forecasted weather attributes per date, per businessEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745b6e12-2339-48f6-8960-628e9187e64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "df_actual = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    weather_date,\n",
    "    weather_type,\n",
    "    temperature,\n",
    "    wind_speed,\n",
    "    precipitation,\n",
    "    businessentityid,\n",
    "    businessentityname\n",
    "FROM gold.weatherdata_daily_historical_openmeteo_nws\n",
    "\"\"\").withColumn(\"weather_date\", to_date(\"weather_date\"))\n",
    "\n",
    "df_forecasted = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    weather_date,\n",
    "    weather_type_forecast,\n",
    "    temperature_forecast,\n",
    "    wind_speed_forecast,\n",
    "    precipitation_forecast,\n",
    "    businessentityid,\n",
    "    businessentityname\n",
    "FROM gold.weatherdata_daily_forecast_nws\n",
    "\"\"\").withColumn(\"weather_date\", to_date(\"weather_date\"))\n",
    "\n",
    "df_weather_combined = df_actual.join(\n",
    "    df_forecasted,\n",
    "    on=[\"weather_date\", \"businessentityid\", \"businessentityname\"],\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "display(df_weather_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10b8f8f-ea48-4791-a0a0-171c74264f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identify Past \"null\" ORDER DAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a8d59b-b57c-4c6a-ad3d-a5764ca5f61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, current_date\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Extract distinct dates from calendar table for the last 24 months and filter out future dates\n",
    "calendar_range = df_calendar.select(\"Date\").filter(\"Date >= date_sub(current_date(), 730) AND Date <= current_date()\")\n",
    "\n",
    "# Convert OrderDate to date\n",
    "df_sales_date = df_sales.withColumn(\"SalesDate\", to_date(\"OrderDate\"))\n",
    "\n",
    "# Get actual sales dates per Site\n",
    "sales_activity = df_sales_date.groupBy(\"SiteID\", \"SalesDate\").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# Join calendar with sales to find dates with no orders\n",
    "closure_candidates = calendar_range.crossJoin(df_sales.select(\"SiteID\").distinct()).withColumnRenamed(\"Date\", \"SalesDate\") \\\n",
    "    .join(sales_activity, on=[\"SiteID\", \"SalesDate\"], how=\"left\") \\\n",
    "    .filter(\"TotalOrders IS NULL\")\n",
    "\n",
    "display(closure_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2992c73c-d8c4-4937-8dfb-bef6a593f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Now Let's CHECK The Weather Conditions On The Same Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1d30d3-0ac2-4d97-8df0-34c6aa02ccb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, current_date\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Extract distinct dates from calendar table for the last 24 months and filter out future dates\n",
    "calendar_range = df_calendar.select(\"Date\").filter(\"Date >= date_sub(current_date(), 730) AND Date <= current_date()\")\n",
    "\n",
    "# Convert OrderDate to date\n",
    "df_sales_date = df_sales.withColumn(\"SalesDate\", to_date(\"OrderDate\"))\n",
    "\n",
    "# Get actual sales dates per Site\n",
    "sales_activity = df_sales_date.groupBy(\"SiteID\", \"SalesDate\").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# Join calendar with sales to find dates with no orders\n",
    "closure_candidates = calendar_range.crossJoin(df_sales.select(\"SiteID\").distinct()).withColumnRenamed(\"Date\", \"SalesDate\") \\\n",
    "    .join(sales_activity, on=[\"SiteID\", \"SalesDate\"], how=\"left\") \\\n",
    "    .filter(\"TotalOrders IS NULL\")\n",
    "\n",
    "# Add weather_type to the resulting dates\n",
    "closure_with_weather = closure_candidates.join(\n",
    "    df_wethr.selectExpr(\"weather_date as SalesDate\", \"weather_type\"),\n",
    "    on=\"SalesDate\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "display(closure_with_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e51a9932-e3c6-43ec-86a8-6b9b0ab51c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Prev Table Shows All Hourls weather_type Which Is Difficult To Interpret For The Daily Weather. Hence, Let's RANK The weather_type Based On It's Severity And Then Get The Overall Daily Weather For The Same Dates As In The Above Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83146341-b44c-4fe8-aa4a-33dfc4bd8a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, current_date, col, when, max as F_max\n",
    "\n",
    "# Extract last 24 months from calendar and filter out future dates\n",
    "calendar_range = df_calendar.select(\"Date\").filter(\"Date >= date_sub(current_date(), 730) AND Date <= current_date()\")\n",
    "\n",
    "# Convert OrderDate to date\n",
    "df_sales_date = df_sales.withColumn(\"SalesDate\", to_date(\"OrderDate\"))\n",
    "\n",
    "# Get distinct SiteID + businessEntityId + businessEntityName\n",
    "distinct_sites = df_sales.select(\"SiteID\", \"businessEntityId\", \"businessEntityName\").distinct()\n",
    "\n",
    "# Get actual sales dates per Site\n",
    "sales_activity = df_sales_date.groupBy(\"SiteID\", \"SalesDate\").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# Join calendar with site list (with businessEntity info), then join to sales_activity\n",
    "closure_candidates = (\n",
    "    calendar_range.crossJoin(distinct_sites)\n",
    "    .withColumnRenamed(\"Date\", \"SalesDate\")\n",
    "    .join(sales_activity, on=[\"SiteID\", \"SalesDate\"], how=\"left\")\n",
    "    .filter(\"TotalOrders IS NULL\")\n",
    ")\n",
    "\n",
    "# Severity mapping for weather types\n",
    "weather_severity = {\n",
    "    \"Clear\": 1,\n",
    "    \"Partly cloudy\": 2,\n",
    "    \"Overcast\": 3,\n",
    "    \"Patchy moderate snow\": 4,\n",
    "    \"Moderate snow\": 5,\n",
    "    \"Ice pellets\": 6,\n",
    "    \"Moderate or heavy snow showers\": 7,\n",
    "    \"Blowing snow\": 8,\n",
    "    \"Heavy snow\": 9,\n",
    "    \"Moderate or heavy rain with thunder\": 10\n",
    "}\n",
    "\n",
    "# Add severity ranking to weather data\n",
    "df_wethr_with_severity = df_wethr.withColumn(\n",
    "    \"severity\",\n",
    "    when(col(\"weather_type\") == \"Clear\", 1)\n",
    "    .when(col(\"weather_type\") == \"Partly cloudy\", 2)\n",
    "    .when(col(\"weather_type\") == \"Overcast\", 3)\n",
    "    .when(col(\"weather_type\") == \"Patchy moderate snow\", 4)\n",
    "    .when(col(\"weather_type\") == \"Moderate snow\", 5)\n",
    "    .when(col(\"weather_type\") == \"Ice pellets\", 6)\n",
    "    .when(col(\"weather_type\") == \"Moderate or heavy snow showers\", 7)\n",
    "    .when(col(\"weather_type\") == \"Blowing snow\", 8)\n",
    "    .when(col(\"weather_type\") == \"Heavy snow\", 9)\n",
    "    .when(col(\"weather_type\") == \"Moderate or heavy rain with thunder\", 10)\n",
    ")\n",
    "\n",
    "# Most severe weather for each date\n",
    "df_weather_daily = df_wethr_with_severity.groupBy(\"weather_date\").agg(\n",
    "    F_max(\"severity\").alias(\"max_severity\")\n",
    ").alias(\"a\").join(\n",
    "    df_wethr_with_severity.alias(\"b\"),\n",
    "    (col(\"a.weather_date\") == col(\"b.weather_date\")) & (col(\"a.max_severity\") == col(\"b.severity\")),\n",
    "    how=\"left\"\n",
    ").select(\"a.weather_date\", \"b.weather_type\").distinct()\n",
    "\n",
    "# Add weather_type to closure candidates\n",
    "closure_with_weather = closure_candidates.join(\n",
    "    df_weather_daily.selectExpr(\"weather_date as SalesDate\", \"weather_type\"),\n",
    "    on=\"SalesDate\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "display(closure_with_weather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa8d309-ff14-4d09-9fdb-2bd6b83e7d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# # STEP 1: Prepare the Historical Weather + Store Closure Dataset\n",
    "\n",
    "We’ll build a labeled dataset where:\n",
    "\t•\t1 = store was closed\n",
    "\t•\t0 = store was open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdae144-a2e6-4dde-9e98-6e388a457eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, col, when\n",
    "\n",
    "# Get all possible SiteID/date combinations (calendar × sites)\n",
    "calendar_range = df_calendar.select(to_date(\"Date\").alias(\"SalesDate\"))\n",
    "sites = df_sales.select(\"SiteID\").distinct()\n",
    "all_site_dates = calendar_range.crossJoin(sites)\n",
    "\n",
    "# Aggregate sales activity: how many orders per site/day\n",
    "df_sales_with_date = df_sales.withColumn(\"SalesDate\", to_date(\"OrderDate\"))\n",
    "sales_activity = df_sales_with_date.groupBy(\"SiteID\", \"SalesDate\").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# Label data: 1 = closed (no orders that day), 0 = open (≥1 order)\n",
    "labeled_data = all_site_dates.join(\n",
    "    sales_activity, on=[\"SiteID\", \"SalesDate\"], how=\"left\"\n",
    ").withColumn(\n",
    "    \"StoreClosedLabel\", when(col(\"TotalOrders\").isNull(), 1).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d1415c7-e36d-4d20-96b3-72a1bd7235d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "site_business_map = (\n",
    "    df_sales\n",
    "    .select(\"SiteID\", \"businessEntityId\", \"businessEntityName\")\n",
    "    .distinct()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7f3502-774e-4980-a1a6-a48e7cbb4411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "labeled_with_business = labeled_data.join(site_business_map, on=\"SiteID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f86c7eb-ba4c-4fc2-acda-f66c99471d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(labeled_with_business.orderBy(\"SalesDate\", \"SiteID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f98e7c-a34b-455f-8f90-0f962f1a1a56",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1750672230924}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_wethr.select(\"weather_type\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd86f66-97b0-428f-b6e7-f3bcb60fdb17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, to_date, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Start from your weather DataFrame: df_wethr\n",
    "df_wethr = df_wethr.withColumn(\"SalesDate\", to_date(col(\"weather_date\")))\n",
    "\n",
    "# Custom mapping for your actual weather types\n",
    "df_wethr = df_wethr.withColumn(\n",
    "    \"severity\",\n",
    "    when(col(\"weather_type\").isin(\"Clear\", \"Mostly Clear\", \"Mostly sunny\", \"Clear and Windy\"), 1)\n",
    "    .when(col(\"weather_type\").isin(\"Partly cloudy\", \"Partly Cloudy\", \"Mostly Cloudy\", \"Cloudy\"), 2)\n",
    "    .when(col(\"weather_type\").isin(\"Haze\", \"Fog/Mist\", \"Unknown\"), 3)\n",
    "    .when(col(\"weather_type\").isin(\"Light Rain\", \"Light rain\", \"Light Rain and Fog/Mist\"), 4)\n",
    "    .when(col(\"weather_type\") == \"Rain\", 5)\n",
    "    .when(col(\"weather_type\").isin(\"Thunderstorms and Rain\", \"Thunderstorms\"), 6)\n",
    "    .when(col(\"weather_type\") == \"Heavy Thunderstorms and Heavy Rain\", 7)\n",
    "    .when(col(\"weather_type\").isin(\"Snow\", \"Light snow\"), 8)\n",
    "    .otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86b962c-8194-4d59-a3fa-88816bbe05c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"SalesDate\").orderBy(col(\"severity\").desc())\n",
    "weather_most_severe = (\n",
    "    df_wethr.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "            .filter(\"row_num = 1\")\n",
    "            .select(\"SalesDate\", \"weather_type\", \"severity\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1671d92-fe62-4f21-8b21-c232f00b804e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_labeled = labeled_with_business.join(\n",
    "    weather_most_severe,\n",
    "    on=\"SalesDate\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"SalesDate\",\n",
    "    \"SiteID\",\n",
    "    \"businessEntityId\",\n",
    "    \"businessEntityName\",      # <-- NEW!\n",
    "    \"StoreClosedLabel\",\n",
    "    \"TotalOrders\",\n",
    "    \"weather_type\",\n",
    "    \"severity\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12296ca5-c8e7-416e-ab86-d61fe8a5c8e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, col, when, current_date, max as F_max, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Calendar range\n",
    "calendar_range = df_calendar.select(to_date(\"Date\").alias(\"SalesDate\")) \\\n",
    "    .filter(\"SalesDate >= date_sub(current_date(), 730) AND SalesDate <= current_date()\")\n",
    "\n",
    "sites = df_sales.select(\"SiteID\").distinct()\n",
    "all_site_dates = calendar_range.crossJoin(sites)\n",
    "\n",
    "# 2. Sales with dates\n",
    "df_sales_with_date = df_sales.withColumn(\"SalesDate\", to_date(\"OrderDate\"))\n",
    "\n",
    "sales_activity = df_sales_with_date.groupBy(\"SiteID\", \"SalesDate\").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# 3. Label closure\n",
    "labeled_data = all_site_dates.join(\n",
    "    sales_activity, on=[\"SiteID\", \"SalesDate\"], how=\"left\"\n",
    ").withColumn(\n",
    "    \"StoreClosedLabel\", when(col(\"TotalOrders\").isNull(), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# --- Add business entity info ---\n",
    "site_business_map = (\n",
    "    df_sales\n",
    "    .select(\"SiteID\", \"businessEntityId\", \"businessEntityName\")\n",
    "    .distinct()\n",
    ")\n",
    "labeled_with_business = labeled_data.join(site_business_map, on=\"SiteID\", how=\"left\")\n",
    "\n",
    "# 4. Weather severity\n",
    "weather_severity = {\n",
    "    \"Clear\": 1, \"Partly cloudy\": 2, \"Overcast\": 3, \"Mist\": 4,\n",
    "    \"Patchy moderate snow\": 5, \"Moderate snow\": 6, \"Ice pellets\": 7,\n",
    "    \"Moderate or heavy snow showers\": 8, \"Blowing snow\": 9, \"Heavy snow\": 10,\n",
    "    \"Moderate rain\": 11, \"Moderate or heavy rain with thunder\": 12,\n",
    "    \"Heavy rain\": 13, \"Thunderstorm\": 14\n",
    "}\n",
    "\n",
    "df_wethr_ranked = df_wethr.withColumn(\"SalesDate\", to_date(\"weather_date\"))\n",
    "severity_col = when(col(\"weather_type\").isNull(), None)\n",
    "for wt, sev in weather_severity.items():\n",
    "    severity_col = severity_col.when(col(\"weather_type\") == wt, sev)\n",
    "severity_col = severity_col.otherwise(0)\n",
    "df_wethr_ranked = df_wethr_ranked.withColumn(\"severity\", severity_col)\n",
    "\n",
    "# 5. Most severe weather per day\n",
    "window_spec = Window.partitionBy(\"SalesDate\").orderBy(col(\"severity\").desc())\n",
    "weather_most_severe = df_wethr_ranked.withColumn(\n",
    "    \"row_num\", row_number().over(window_spec)\n",
    ").filter(\"row_num = 1\").select(\"SalesDate\", \"weather_type\", \"severity\")\n",
    "\n",
    "# 6. Final join\n",
    "df_final_labeled = labeled_with_business.join(\n",
    "    weather_most_severe,\n",
    "    on=\"SalesDate\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"SalesDate\",\n",
    "    \"SiteID\",\n",
    "    \"businessEntityId\",\n",
    "    \"businessEntityName\",      # <-- Now included!\n",
    "    \"StoreClosedLabel\",\n",
    "    \"TotalOrders\",\n",
    "    \"weather_type\",\n",
    "    \"severity\"\n",
    ")\n",
    "\n",
    "# Show result\n",
    "display(df_final_labeled.orderBy(\"SalesDate\", \"SiteID\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432bdc75-cbd0-45c1-a047-df92214877ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "display(df_final_labeled.orderBy(\"SalesDate\", \"SiteID\"))\n",
    "\n",
    "# Check for NULLs\n",
    "null_counts = df_final_labeled.select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_final_labeled.columns]\n",
    ")\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a5c9ef-02ef-4c3b-a301-bc99c26a18f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Remove rows where SiteID is NULL\n",
    "df_final_labeled = df_final_labeled.filter(F.col(\"SiteID\").isNotNull())\n",
    "\n",
    "# Fill NA for TotalOrders\n",
    "df_final_labeled = df_final_labeled.fillna({\"TotalOrders\": 0})\n",
    "\n",
    "# Standardize any remaining NAs\n",
    "df_final_labeled = df_final_labeled.na.fill(\"Unknown\")\n",
    "\n",
    "display(df_final_labeled.orderBy(\"SalesDate\", \"SiteID\"))\n",
    "\n",
    "# Check for NULLs\n",
    "null_counts = df_final_labeled.select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_final_labeled.columns]\n",
    ")\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aadc5b2-8fed-4bcc-a785-bfe022f1c0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5d3943-4473-4455-9cf6-567b4154a81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Analyze Closure Patterns(Count Entity By Closure)\n",
    "\n",
    "display(df_final_labeled.groupBy(\"businessEntityName\", \"StoreClosedLabel\")\n",
    "         .count().orderBy(\"businessEntityName\", \"StoreClosedLabel\"))\n",
    "display(df_final_labeled.groupBy(\"weather_type\", \"StoreClosedLabel\")\n",
    "         .count().orderBy(\"weather_type\", \"StoreClosedLabel\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c08d3bd-1e28-438e-9629-9bc4c1352628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# My **aggregated closure summary** is **FINE for reporting and analysis**—you now see, by business and by weather type, where closures (label = 1) are more frequent. Here’s how to **interpret and use this information**—plus, what to do next:\n",
    "\n",
    "---\n",
    "\n",
    "## **How to Interpret This Table**\n",
    "\n",
    "### **1. By Business Entity**\n",
    "\n",
    "* **Most entities (e.g., \"Namak Mirch Grocers\", \"Lulu's Diner II\", etc.) are open 99.8%+ of the time** (730 open, 1 closed = one closure per 2 years, likely during severe weather).\n",
    "* **Entities like ParadiseGifts4/5/7 and SubParadiseGifts1 have much higher closure rates** (e.g., 409–422 closed days vs. 309–1053 open), suggesting either:\n",
    "\n",
    "  * **Less consistent sales reporting**\n",
    "  * **A store that’s open seasonally or with many holidays**\n",
    "  * **Data issues or special business rules**\n",
    "* **\"OVVI Automation\"** has many sites/entries (8030 open, 11 closed)—likely a test/demo or multi-location business.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. By Weather Type**\n",
    "\n",
    "* **Most closures happen on clear or partly cloudy days!**\n",
    "\n",
    "  * **Interpretation:**\n",
    "\n",
    "    * **Closures aren’t only weather-related.**\n",
    "    * May reflect holidays, low season, or missing/erroneous weather data.\n",
    "    * Need to check if “closed” labels are sometimes due to other factors (e.g., holidays, off-season).\n",
    "* **Higher closure rates in “Rain”, “Light rain”, “Cloudy”, “Mostly sunny”**: Could indicate weather impact—but not exclusively.\n",
    "* **Very few closures on “Thunderstorms”, “Light snow”, etc.:**\n",
    "\n",
    "  * **Possible reasons:**\n",
    "\n",
    "    * Extreme weather is rare in your data’s geography.\n",
    "    * The store closes for other reasons more often than for truly severe weather.\n",
    "    * Or: Weather reporting might not always match the true local conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a0acca-76e5-4653-9592-05b6ccf3d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Cross Tab Entity By weather_type And Closure\n",
    "# Count closures (StoreClosedLabel = 1) by business and weather\n",
    "df_bad_weather = df_final_labeled.filter(col(\"StoreClosedLabel\") == 1)\n",
    "\n",
    "display(\n",
    "    df_bad_weather.groupBy(\"businessEntityName\", \"weather_type\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"count\", ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d9b829-e73e-401d-b34c-aa3441751bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "# Add day_of_week (1=Sunday, 7=Saturday)\n",
    "df_final_labeled = df_final_labeled.withColumn(\n",
    "    \"day_of_week\", dayofweek(\"SalesDate\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d1ca1f-967b-47b8-880d-5b8f3dbc521b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Map day_of_week number to day name\n",
    "df_final_labeled = (\n",
    "    df_final_labeled.withColumn(\n",
    "        \"day_name\",\n",
    "        when(col(\"day_of_week\") == 1, \"Sunday\")\n",
    "        .when(col(\"day_of_week\") == 2, \"Monday\")\n",
    "        .when(col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "        .when(col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "        .when(col(\"day_of_week\") == 5, \"Thursday\")\n",
    "        .when(col(\"day_of_week\") == 6, \"Friday\")\n",
    "        .when(col(\"day_of_week\") == 7, \"Saturday\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103c6f5c-d56a-406d-a651-1fa9f69afb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Closed Vs Open Rate By weekday\n",
    "display(\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"day_of_week\", \"StoreClosedLabel\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"day_of_week\", \"StoreClosedLabel\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327c2f12-bf68-4863-b38f-0661031b47b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## With Day Nam\n",
    "display(\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"day_name\", \"StoreClosedLabel\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"day_name\", \"StoreClosedLabel\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47969339-2c39-40a1-9586-cb17dca0d65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Closed Vs Open By Mont\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "df_final_labeled = df_final_labeled.withColumn(\n",
    "    \"month\", month(\"SalesDate\")\n",
    ")\n",
    "\n",
    "display(df_final_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a415d8-f667-48c9-93a2-9d9bc0344fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Join Calendar\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_final_labeled = (\n",
    "    df_final_labeled\n",
    "    .join(\n",
    "        df_calendar.select(\n",
    "            col(\"Date\").alias(\"SalesDate\"),\n",
    "            \"WeekDayName\",\n",
    "            \"MonthName\",\n",
    "            \"Month\",\n",
    "            \"Year\",\n",
    "            \"IsWeekend\",\n",
    "            \"IsHoliday\",\n",
    "            \"HolidayName\"\n",
    "        ),\n",
    "        on=\"SalesDate\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91e3a85-56f5-4899-84d6-689980a39fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Use Calendar Fat For More Group Analysis\n",
    "\n",
    "display(\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"WeekDayName\", \"StoreClosedLabel\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"WeekDayName\", \"StoreClosedLabel\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "890d9fa3-0051-4afc-bbf3-e6c7e6984f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Holiday Weekend Closure\n",
    "display(\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"IsHoliday\", \"StoreClosedLabel\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"IsHoliday\", \"StoreClosedLabel\")\n",
    ")\n",
    "display(\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"IsWeekend\", \"StoreClosedLabel\")\n",
    "        .count()\n",
    "        .orderBy(\"businessEntityName\", \"IsWeekend\", \"StoreClosedLabel\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e0845c-3dac-44bd-b50f-fdb8a2c3876f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interpretatio\n",
    "## Great! These two tables tell me a **lot** about which business entities are prone to closing on holidays and weekends vs. regular days.\n",
    "\n",
    "---\n",
    "\n",
    "## **What Can We Interpret?**\n",
    "\n",
    "### **A. Holiday Closure Patterns**\n",
    "\n",
    "* **Most “classic” restaurants** (Altoona, Carniceria Sonora, Demo, Lulu's, Namak Mirch, Green Bawarchi Katy) **rarely close on holidays**—holiday rows show almost exclusively 0’s (open), with maybe one or two closures across all years.\n",
    "* **“Paradise” and “OVVI Automation” chains** have a much higher closure rate on holidays:\n",
    "\n",
    "  * ParadiseGift5, ParadiseGifts3/4/7, SubParadiseGifts1 all show **nonzero closure counts on holidays**. For example, ParadiseGift5 has 26 closures on holidays, and even on non-holidays, closure counts are high (e.g., 396 closures vs 299 open).\n",
    "  * These stores are generally **much more likely to close**—potentially due to operational policy, seasonality, or being “gift”/specialty shops.\n",
    "\n",
    "### **B. Weekend Closure Patterns**\n",
    "\n",
    "* **Again, classic restaurants and grocers are open on nearly all weekends** (very few closures with IsWeekend = 1).\n",
    "* **Paradise and SubParadise** shops, plus some OVVI units, show a significant number of closures on weekends (compare open vs. closed counts for IsWeekend = 1).\n",
    "* **Exceptionally high closure rates** for “Paradise” entities on both weekends and holidays compared to the food businesses.\n",
    "\n",
    "---\n",
    "## **Summary Table Example**\n",
    "\n",
    "| Entity              | Holiday Closure Rate | Weekend Closure Rate | Non-Holiday/Weekend Rate |\n",
    "| ------------------- | -------------------- | -------------------- | ------------------------ |\n",
    "| Namak Mirch Grocers | Very Low             | Very Low             | Very Low                 |\n",
    "| ParadiseGift5       | High                 | High                 | High                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3915281-f1bb-42ea-9a2a-370ed6a42446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1934a59-09fc-42cb-9ae3-576aaeba19e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27f0eae-8630-4f5d-a92a-56029f87b715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Rule BASED Store Closure Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79d4678-219f-49ef-88ef-54435b918680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Ingest & Materialize a “Master” DataFrame\n",
    "\n",
    "Join all six tables on the date dimension so that every row is one calendar‐date for “Namak Mirch Grocers,” with sales, orders, weather (hist + forecast), and calendar attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5427e67c-5f4f-4158-8e13-79a110bb4233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StoreClosureSevereWeather\").getOrCreate()\n",
    "\n",
    "# 1. Load tables filtered to Namak Mirch Grocers\n",
    "sales_line = (\n",
    "    spark.table(\"gold.salestransskufact\")\n",
    "         .filter(F.col(\"businessEntityName\") == \"Namak Mirch Grocers\")\n",
    "         .select(\n",
    "             F.to_date(\"OrderDate\").alias(\"date\"),\n",
    "             F.col(\"SalesQuantity\"),\n",
    "             F.col(\"NetSalesPrice\")\n",
    "         )\n",
    ")\n",
    "\n",
    "sales_order = (\n",
    "    spark.table(\"gold.salestransorderfact\")\n",
    "         .filter(F.col(\"businessEntityName\") == \"Namak Mirch Grocers\")\n",
    "         .select(\n",
    "             F.to_date(\"OrderDate\").alias(\"date\"),\n",
    "             F.col(\"TotalSalesQuantity\"),\n",
    "             F.col(\"TotalSalesAmount\")\n",
    "         )\n",
    ")\n",
    "\n",
    "weather_hist = (\n",
    "    spark.table(\"gold.weatherdata_daily_historical_openmeteo_nws\")\n",
    "         .filter(F.col(\"businessEntityName\") == \"Namak Mirch Grocers\")\n",
    "         .select(\n",
    "             F.to_date(\"weather_date\").alias(\"date\"),\n",
    "             \"weather_type\", \"precipitation\", \"wind_speed\"\n",
    "         )\n",
    ")\n",
    "\n",
    "weather_forecast = (\n",
    "    spark.table(\"gold.weatherdata_daily_forecast_nws\")\n",
    "         .filter(F.col(\"businessEntityName\") == \"Namak Mirch Grocers\")\n",
    "         .select(\n",
    "             F.to_date(\"weather_date\").alias(\"date\"),\n",
    "             F.col(\"weather_type_forecast\").alias(\"weather_type\"),\n",
    "             F.col(\"precipitation_forecast\").alias(\"precipitation\"),\n",
    "             F.col(\"wind_speed_forecast\").alias(\"wind_speed\")\n",
    "         )\n",
    ")\n",
    "\n",
    "calendar = spark.table(\"gold.calendar\").select(\n",
    "    F.to_date(\"Date\").alias(\"date\"),\n",
    "    \"IsWeekend\", \"IsHoliday\", \"WeekDayName\", \"MonthName\", \"Year\"\n",
    ")\n",
    "\n",
    "products = spark.table(\"gold.product\") \\\n",
    "                .filter(F.col(\"businessEntityName\") == \"Namak Mirch Grocers\") \\\n",
    "                .select(\"ProductID\", \"ProductName\", \"activeFlag\")\n",
    "\n",
    "\n",
    "# 2. Aggregate daily activity\n",
    "daily_sales = (\n",
    "    sales_line.groupBy(\"date\")\n",
    "        .agg(\n",
    "            F.sum(\"SalesQuantity\").alias(\"total_units_sold\"),\n",
    "            F.sum(\"NetSalesPrice\").alias(\"total_revenue\")\n",
    "        )\n",
    ")\n",
    "\n",
    "daily_orders = (\n",
    "    sales_order.groupBy(\"date\")\n",
    "        .agg(\n",
    "            F.sum(\"TotalSalesQuantity\").alias(\"order_units\"),\n",
    "            F.sum(\"TotalSalesAmount\").alias(\"order_revenue\")\n",
    "        )\n",
    ")\n",
    "\n",
    "daily_activity = (\n",
    "    calendar\n",
    "      .join(daily_sales, \"date\", \"left\")\n",
    "      .join(daily_orders, \"date\", \"left\")\n",
    "      .na.fill(0, subset=[\"total_units_sold\", \"total_revenue\", \"order_units\", \"order_revenue\"])\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Define UDF to flag severe weather\n",
    "severe_types = F.array(\n",
    "    *[F.lit(w) for w in [\"Thunderstorm\", \"Heavy Rain\", \"Storm\", \"Hurricane\", \"Snow\", \"Extreme\"]]\n",
    ")\n",
    "flag_severe = (\n",
    "    (F.col(\"precipitation\") >= F.lit(5.0))        # ≥5 mm rain\n",
    "    | (F.col(\"wind_speed\") >= F.lit(20.0))         # ≥20 m/s wind\n",
    "    | F.array_contains(severe_types, F.col(\"weather_type\"))\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Build historical and forecast views with severe‐weather flag\n",
    "weather_h_flag = (\n",
    "    weather_hist\n",
    "      .withColumn(\"is_severe\", flag_severe.cast(\"boolean\"))\n",
    "      .select(\"date\", \"is_severe\")\n",
    ")\n",
    "\n",
    "weather_f_flag = (\n",
    "    weather_forecast\n",
    "      .withColumn(\"is_severe\", flag_severe.cast(\"boolean\"))\n",
    "      .select(\"date\", \"is_severe\")\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Identify actual closures: no sales AND severe historical weather\n",
    "closures_historical = (\n",
    "    daily_activity\n",
    "      .join(weather_h_flag, \"date\", \"left\")\n",
    "      .filter(\n",
    "          (F.col(\"total_revenue\") == 0)  # no revenue → no open transactions\n",
    "          & (F.col(\"is_severe\") == True) # flagged severe weather\n",
    "      )\n",
    "      .select(\n",
    "          \"date\", \"WeekDayName\", \"MonthName\", \"Year\",\n",
    "          \"IsWeekend\", \"IsHoliday\",\n",
    "          F.col(\"is_severe\").alias(\"severe_weather\"),\n",
    "          F.col(\"total_revenue\"),\n",
    "          F.col(\"order_revenue\")\n",
    "      )\n",
    "      .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Identify forecasted closure risks: forecast severe AND historical low baseline\n",
    "#    e.g. if past same weekday or month typically low sales\n",
    "#    Here we simply mark days with forecast severe weather and zero forecasted baseline\n",
    "closures_forecast = (\n",
    "    daily_activity\n",
    "      .join(weather_f_flag, \"date\", \"left\")\n",
    "      .filter(F.col(\"is_severe\") == True)\n",
    "      .select(\n",
    "          \"date\", \"WeekDayName\", \"MonthName\", \"Year\",\n",
    "          \"IsWeekend\", \"IsHoliday\",\n",
    "          F.col(\"is_severe\").alias(\"forecast_severe\"),\n",
    "          \"total_revenue\", \"order_revenue\"\n",
    "      )\n",
    "      .orderBy(\"date\")\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Show results\n",
    "print(\"=== Historical Store Closures Attributable to Severe Weather ===\")\n",
    "display(closures_historical)\n",
    "\n",
    "print(\"=== Forecasted Store Closure Risk Due to Severe Weather ===\")\n",
    "display(closures_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfda70f-e1d3-44a6-8846-1eca09fa94a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_order = (\n",
    "    spark.table(\"gold.salestransorderfact\")\n",
    "         .select(\n",
    "             F.to_date(\"OrderDate\").alias(\"date\"),\n",
    "             F.col(\"TotalSalesQuantity\"),\n",
    "             F.col(\"TotalSalesAmount\"),\n",
    "             \"businessEntityId\",\n",
    "             \"businessEntityName\"\n",
    "         )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac98444-c763-4321-86a3-ede776891381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify the column names in the sales_order DataFrame\n",
    "sales_order_columns = sales_order.columns\n",
    "print(sales_order_columns)\n",
    "\n",
    "# Mapping of businessEntityId/name to each date (and possibly SiteID if needed)\n",
    "entity_map = (\n",
    "    sales_order\n",
    "    .select(\"date\", \"businessEntityId\", \"businessEntityName\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "display(entity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4d4031-3aee-4df8-99d4-7e74681fb777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark.table(\"gold.weatherdata_daily_historical_openmeteo_nws\").columns)\n",
    "print(spark.table(\"gold.salestransskufact\").columns)\n",
    "print(spark.table(\"gold.salestransorderfact\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e92a99-0c9f-47db-ba43-f17d02619a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, countDistinct, col, when, current_date, max as F_max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Prepare calendar dates (for last 24 months)\n",
    "calendar_range = (\n",
    "    spark.table(\"gold.calendar\")\n",
    "    .select(to_date(\"Date\").alias(\"date\"))\n",
    "    .filter(\"date >= date_sub(current_date(), 730) AND date <= current_date()\")\n",
    ")\n",
    "\n",
    "# 2. Get all (businessEntityId, businessEntityName, SiteID) pairs\n",
    "sites = (\n",
    "    spark.table(\"gold.salestransskufact\")\n",
    "    .select(\"businessEntityId\", \"businessEntityName\", \"SiteID\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# 3. Get all combinations (cross join)\n",
    "all_site_dates = (\n",
    "    calendar_range.crossJoin(sites)\n",
    ")\n",
    "\n",
    "# 4. Get sales activity per site/date\n",
    "sales = (\n",
    "    spark.table(\"gold.salestransskufact\")\n",
    "    .withColumn(\"date\", to_date(\"OrderDate\"))\n",
    ")\n",
    "\n",
    "sales_activity = sales.groupBy(\n",
    "    \"businessEntityId\", \"businessEntityName\", \"SiteID\", \"date\"\n",
    ").agg(\n",
    "    countDistinct(\"OrderID\").alias(\"TotalOrders\")\n",
    ")\n",
    "\n",
    "# 5. Label store closures (1 = closed, 0 = open)\n",
    "labeled_data = (\n",
    "    all_site_dates.join(\n",
    "        sales_activity,\n",
    "        on=[\"businessEntityId\", \"businessEntityName\", \"SiteID\", \"date\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\"StoreClosedLabel\", when(col(\"TotalOrders\").isNull(), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# 6. Prepare weather: best daily weather per business entity\n",
    "weather = (\n",
    "    spark.table(\"gold.weatherdata_daily_historical_openmeteo_nws\")\n",
    "    .select(\n",
    "        \"businessEntityId\", \"businessEntityName\",\n",
    "        to_date(\"weather_date\").alias(\"date\"),\n",
    "        \"weather_type\", \"precipitation\", \"wind_speed\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add severity ranking (optional)\n",
    "weather_severity = {\n",
    "    \"Clear\": 1, \"Partly cloudy\": 2, \"Overcast\": 3, \"Mist\": 4, \"Patchy moderate snow\": 5,\n",
    "    \"Moderate snow\": 6, \"Ice pellets\": 7, \"Moderate or heavy snow showers\": 8,\n",
    "    \"Blowing snow\": 9, \"Heavy snow\": 10, \"Moderate rain\": 11,\n",
    "    \"Moderate or heavy rain with thunder\": 12, \"Heavy rain\": 13, \"Thunderstorm\": 14\n",
    "}\n",
    "# UDF to map type to severity (or use a when-chain as before)\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from itertools import chain\n",
    "\n",
    "# ... inside your code after defining weather_severity:\n",
    "mapping_expr = create_map([lit(x) for x in chain.from_iterable(weather_severity.items())])\n",
    "weather = weather.withColumn(\"severity\", mapping_expr[col(\"weather_type\")])\n",
    "\n",
    "\n",
    "# For each entity/date, pick the worst (max) severity and its weather_type\n",
    "window = Window.partitionBy(\"businessEntityId\", \"businessEntityName\", \"date\").orderBy(col(\"severity\").desc())\n",
    "weather_most_severe = (\n",
    "    weather.withColumn(\"row_num\", F.row_number().over(window))\n",
    "           .filter(\"row_num = 1\")\n",
    "           .select(\"businessEntityId\", \"businessEntityName\", \"date\", \"weather_type\", \"severity\")\n",
    ")\n",
    "\n",
    "# 7. Join weather to labeled data\n",
    "df_final = (\n",
    "    labeled_data.join(\n",
    "        weather_most_severe,\n",
    "        on=[\"businessEntityId\", \"businessEntityName\", \"date\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\"date\", \"businessEntityId\", \"businessEntityName\", \"SiteID\", \"StoreClosedLabel\", \"TotalOrders\", \"weather_type\", \"severity\")\n",
    "    .orderBy(\"date\", \"businessEntityId\", \"SiteID\")\n",
    ")\n",
    "\n",
    "display(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582f92bc-123d-4426-b452-e13cbc779bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "calendar = (\n",
    "    spark.table(\"gold.calendar\")\n",
    "         .select(\n",
    "             F.to_date(\"Date\").alias(\"date\"),\n",
    "             \"IsWeekend\", \"IsHoliday\", \"WeekDayName\", \"MonthName\", \"Year\"\n",
    "         )\n",
    ")\n",
    "\n",
    "df_final_labeled = (\n",
    "    df_final.join(calendar, on=\"date\", how=\"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853e08ed-328e-43fe-9c41-ad25f92ea7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "null_counts = df_final_labeled.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df_final_labeled.columns]\n",
    ")\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87b5c0d-80b9-4a69-9c21-28b2dee1ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_labeled = df_final_labeled.fillna({'severity': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e719eb-2828-4e54-aa5e-8449ab16a4d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_labeled = df_final_labeled.fillna({'weather_type': 'Unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a87f84-f794-4788-9d31-449358351567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_labeled = df_final_labeled.filter(col(\"SiteID\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d007173-b1c1-452f-b05f-167bba7c4c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Compute Closure Rates\n",
    "We’ll calculate closure rate per entity across:\n",
    "\n",
    "Weekdays\n",
    "\n",
    "IsHoliday\n",
    "\n",
    "IsWeekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ca62e1-d7b0-4119-92b0-618323294280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Closure Rate by WeekDayName\n",
    "\n",
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "weekday_closure_rate = (\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"WeekDayName\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_days\"),\n",
    "        count(when(col(\"StoreClosedLabel\") == 1, True)).alias(\"closed_days\")\n",
    "    )\n",
    "    .withColumn(\"closure_rate\", col(\"closed_days\") / col(\"total_days\"))\n",
    ")\n",
    "\n",
    "display(weekday_closure_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d398edf5-dee2-400c-99a9-e412d0da7f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Closure Rate by IsHoliday\n",
    "holiday_closure_rate = (\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"IsHoliday\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_days\"),\n",
    "        count(when(col(\"StoreClosedLabel\") == 1, True)).alias(\"closed_days\")\n",
    "    )\n",
    "    .withColumn(\"closure_rate\", col(\"closed_days\") / col(\"total_days\"))\n",
    ")\n",
    "\n",
    "display(holiday_closure_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44bf8c70-e1d3-4406-b34f-7bbe941f7597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Closure Rate by IsWeekend\n",
    "weekend_closure_rate = (\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"IsWeekend\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_days\"),\n",
    "        count(when(col(\"StoreClosedLabel\") == 1, True)).alias(\"closed_days\")\n",
    "    )\n",
    "    .withColumn(\"closure_rate\", col(\"closed_days\") / col(\"total_days\"))\n",
    ")\n",
    "\n",
    "display(weekend_closure_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24aaf3bc-8538-455e-9c7f-c33c769bd8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Excellent! You’ve now successfully computed **closure rates** by:\n",
    "\n",
    "* `WeekDayName`\n",
    "* `IsHoliday`\n",
    "* `IsWeekend`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Key Findings (Interpretation)**\n",
    "\n",
    "### \uD83C\uDFEA 1. **Classic Retailers & Restaurants**\n",
    "\n",
    "Entities like **Namak Mirch Grocers**, **Demo Restaurant**, **Carniceria Sonora #4**, **Green Bawarchi Katy**:\n",
    "\n",
    "* Closure rates on weekends and holidays are **very low** (mostly < 5%)\n",
    "* These stores are clearly **operational almost daily**, with minimal exceptions.\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83C\uDF81 2. **Gift Stores (Paradise*, SubParadise*)\\*\\*\n",
    "\n",
    "Entities like **ParadiseGift5**, **ParadiseGifts3/4/7**, **SubParadiseGifts1**:\n",
    "\n",
    "* Show **extremely high closure rates**, sometimes up to **99.5%** on weekends/holidays\n",
    "* Also notable weekday closures, especially on **Mondays, Tuesdays, Fridays**\n",
    "* Behavior suggests **deliberate, possibly seasonal/weekend-based closure policies**\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83C\uDFE2 3. **OVVI Automation**\n",
    "\n",
    "* Has **100% closure rate** across all days — possibly:\n",
    "\n",
    "  * A non-retail automation/testing entity\n",
    "  * Or a data quality error (check if these represent actual store sites)\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCCA Summary Table Snapshot\n",
    "\n",
    "| Entity              | Holiday Closure | Weekend Closure | Weekday (avg) Closure |\n",
    "| ------------------- | --------------- | --------------- | --------------------- |\n",
    "| Namak Mirch Grocers | 0%              | \\~0.4%          | \\~1.9% max            |\n",
    "| Green Bawarchi Katy | \\~7%            | \\~2%            | Mostly <2.5%          |\n",
    "| ParadiseGift5       | 72%             | 99%             | \\~38–41%              |\n",
    "| SubParadiseGifts1   | 83%             | 99.2%           | \\~68%+ on avg         |\n",
    "| ParadiseGifts4      | 84.7%           | 99.7%           | \\~69%+ on avg         |\n",
    "| OVVI Automation     | 100%            | 100%            | 100% (all days)       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dde7542-e8ce-4544-960d-21119061c2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Compute Closure Rate by IsHoliday x Weather_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d266c0-8b90-4ea3-8c1b-9da91c89bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holiday_weather_closure = (\n",
    "    df_final_labeled.groupBy(\"businessEntityName\", \"IsHoliday\", \"weather_type\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_days\"),\n",
    "        count(when(col(\"StoreClosedLabel\") == 1, True)).alias(\"closed_days\")\n",
    "    )\n",
    "    .withColumn(\"closure_rate\", col(\"closed_days\") / col(\"total_days\"))\n",
    "    .orderBy(\"businessEntityName\", \"IsHoliday\", \"closure_rate\", ascending=False)\n",
    ")\n",
    "\n",
    "display(holiday_weather_closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26eea25a-b1c4-4c28-be2d-b0bfb4b03ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, when\n",
    "\n",
    "# Standardize weather_type\n",
    "df = df_final_labeled.withColumn(\n",
    "    \"weather_type_cleaned\",\n",
    "    lower(regexp_replace(\"weather_type\", \"[-_]\", \" \"))\n",
    ")\n",
    "\n",
    "# Example: group weather\n",
    "df = df.withColumn(\n",
    "    \"weather_grouped\",\n",
    "    when(col(\"weather_type_cleaned\").rlike(\"snow\"), \"Snow\")\n",
    "    .when(col(\"weather_type_cleaned\").rlike(\"rain|thunderstorm\"), \"Rain\")\n",
    "    .when(col(\"weather_type_cleaned\").rlike(\"clear|sunny\"), \"Clear\")\n",
    "    .when(col(\"weather_type_cleaned\").rlike(\"cloudy\"), \"Cloudy\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4fdb12-88a2-4cee-8114-0b4656e7e02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2a3098-9476-47e4-9185-39051428e349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Add Binary Flags for Weather\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"is_rain\", when(col(\"weather_grouped\") == \"Rain\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"is_snow\", when(col(\"weather_grouped\") == \"Snow\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"is_clear\", when(col(\"weather_grouped\") == \"Clear\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"is_cloudy\", when(col(\"weather_grouped\") == \"Cloudy\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"is_other_weather\", when(col(\"weather_grouped\") == \"Other\", 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4de9f6e-af2c-4fac-9af8-5f7bd6f0517a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Extract & Add Numerical Month (Optional)\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "df = df.withColumn(\"month_num\", month(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95b3e9b-7fd1-4aac-998b-089f98492265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Add Rolling Lag Features\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, avg\n",
    "\n",
    "window_spec = Window.partitionBy(\"businessEntityId\").orderBy(\"date\")\n",
    "\n",
    "# Previous day's closure\n",
    "df = df.withColumn(\"prev_day_closed\", lag(\"StoreClosedLabel\").over(window_spec))\n",
    "\n",
    "# 3-day rolling average of closures\n",
    "rolling_window = window_spec.rowsBetween(-3, -1)\n",
    "df = df.withColumn(\"rolling_3day_closure_rate\", avg(\"StoreClosedLabel\").over(rolling_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07a1c16-e05f-4e37-b792-4cd9d01c0400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Optional - One-Hot Encode WeekDayName / MonthName\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"WeekDayName\", outputCol=\"WeekDayIndex\")\n",
    "encoder = OneHotEncoder(inputCols=[\"WeekDayIndex\"], outputCols=[\"WeekDayVec\"])\n",
    "\n",
    "# Fit + transform in pipeline if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9abab3-b511-46e9-b011-eeac44c65f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b387539-bf4e-41f6-9326-103c770157db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add both StringIndexers to your DataFrame\n",
    "# Step 1: StringIndexers\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"WeekDayName\", outputCol=\"WeekDayName_index\", handleInvalid=\"keep\"),\n",
    "    StringIndexer(inputCol=\"MonthName\", outputCol=\"MonthName_index\", handleInvalid=\"keep\")\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a76e0d-87bd-4d7e-b275-b184f5416f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Now re-check for nulls\n",
    "\n",
    "# Redefine feature_cols\n",
    "feature_cols = [\n",
    "    \"TotalOrders\",\n",
    "    \"IsWeekend\",\n",
    "    \"IsHoliday\",\n",
    "    \"month_num\",\n",
    "    \"prev_day_closed\",\n",
    "    \"rolling_3day_closure_rate\",\n",
    "    \"is_rain\",\n",
    "    \"is_snow\",\n",
    "    \"is_clear\",\n",
    "    \"is_cloudy\",\n",
    "    \"is_other_weather\",\n",
    "    \"WeekDayName_index\",\n",
    "    \"MonthName_index\"\n",
    "]\n",
    "\n",
    "# Check for nulls in selected features\n",
    "df.select([\n",
    "    col(c).isNull().alias(c + \"_isNull\") for c in feature_cols\n",
    "]).summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d17aba-0004-4d2a-a67e-ecf7a07f0793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb5ba09-4660-4fb7-be2f-35ba93aaf2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Skip re-creation if columns exists\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = []\n",
    "\n",
    "if \"WeekDayName_index\" not in df.columns:\n",
    "    indexers.append(StringIndexer(inputCol=\"WeekDayName\", outputCol=\"WeekDayName_index\", handleInvalid=\"keep\"))\n",
    "\n",
    "if \"MonthName_index\" not in df.columns:\n",
    "    indexers.append(StringIndexer(inputCol=\"MonthName\", outputCol=\"MonthName_index\", handleInvalid=\"keep\"))\n",
    "\n",
    "if indexers:\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff1a665-ebd6-4d91-82b8-266ebed2ec41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select([\n",
    "    col(c).isNull().alias(c + \"_isNull\") for c in feature_cols\n",
    "]).summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad27569-7674-40d2-b049-f035421bdd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d81827-23e7-486e-8ece-7ce38c0a372d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_cols = [c for c in feature_cols if c not in df.columns]\n",
    "print(\"Missing columns:\", missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d42e0241-5a23-4433-b025-6e38411db29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select([\n",
    "    col(c).isNull().alias(f\"{c}_isNull\") for c in feature_cols\n",
    "]).summary(\"count\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11244df-873a-40e4-9732-78699b6eac84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# --- 1. Cast all flag columns to integer ---\n",
    "for flag_col in [\"IsHoliday\", \"IsWeekend\"]:\n",
    "    df = df.withColumn(\n",
    "        flag_col,\n",
    "        when(\n",
    "            (col(flag_col) == True) | \n",
    "            (col(flag_col) == \"true\") | \n",
    "            (col(flag_col) == \"True\") | \n",
    "            (col(flag_col) == 1), 1\n",
    "        ).otherwise(0).cast(\"int\")\n",
    "    )\n",
    "\n",
    "# --- 2. Fill missing values for ALL potential feature columns ---\n",
    "feature_cols = [\n",
    "    \"TotalOrders\",\n",
    "    \"IsWeekend\",\n",
    "    \"IsHoliday\",\n",
    "    \"month_num\",\n",
    "    \"prev_day_closed\",\n",
    "    \"rolling_3day_closure_rate\",\n",
    "    \"is_rain\",\n",
    "    \"is_snow\",\n",
    "    \"is_clear\",\n",
    "    \"is_cloudy\",\n",
    "    \"is_other_weather\",\n",
    "    \"WeekDayName_index\",\n",
    "    \"MonthName_index\"\n",
    "]\n",
    "\n",
    "# Create comprehensive fill dictionary\n",
    "fill_dict = {col: 0 for col in feature_cols}\n",
    "df = df.fillna(fill_dict)\n",
    "\n",
    "# --- 3. StringIndexer for WeekDayName and MonthName ---\n",
    "indexers = []\n",
    "if \"WeekDayName_index\" not in df.columns:\n",
    "    indexers.append(StringIndexer(inputCol=\"WeekDayName\", outputCol=\"WeekDayName_index\", handleInvalid=\"keep\"))\n",
    "if \"MonthName_index\" not in df.columns:\n",
    "    indexers.append(StringIndexer(inputCol=\"MonthName\", outputCol=\"MonthName_index\", handleInvalid=\"keep\"))\n",
    "\n",
    "if indexers:\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "\n",
    "# --- 4. Ensure all feature columns are double type and handle nulls/NaNs ---\n",
    "for colname in feature_cols:\n",
    "    # Handle potential nulls from casting\n",
    "    df = df.withColumn(\n",
    "        colname, \n",
    "        when(col(colname).isNull() | isnan(col(colname)), 0.0)\n",
    "        .otherwise(col(colname).cast(\"double\"))\n",
    ")\n",
    "\n",
    "# --- 5. Final null check ---\n",
    "print(\"Final null counts:\")\n",
    "null_counts = df.select([\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) \n",
    "    for c in feature_cols\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "# --- 6. Assemble feature vector with safe handling ---\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"  # Critical: Handle any unexpected values\n",
    ")\n",
    "\n",
    "df_model_ready = assembler.transform(df)\n",
    "\n",
    "# --- 7. Verify features before training ---\n",
    "print(\"Sample features:\")\n",
    "df_model_ready.select(\"features\", \"StoreClosedLabel\").show(5, truncate=False)\n",
    "\n",
    "# Now safe to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a3fca8-50d1-4f46-969a-2e8fde91f5d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for nulls in label column\n",
    "label_null_count = df_model_ready.filter(col(\"StoreClosedLabel\").isNull()).count()\n",
    "print(f\"Null labels: {label_null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b40fb3-53cf-4e5f-b67b-4197168f0f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_model_ready.select(feature_cols).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17d37db-63f8-46b0-a444-8ea71d1051ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if indexers produced expected values\n",
    "if \"WeekDayName_index\" in feature_cols:\n",
    "    df_model_ready.groupBy(\"WeekDayName\", \"WeekDayName_index\").count().show()\n",
    "if \"MonthName_index\" in feature_cols:\n",
    "    df_model_ready.groupBy(\"MonthName\", \"MonthName_index\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52472b50-1dff-4376-9ad2-f7393b894833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bbcda5-4d29-49ee-9603-bf685883195f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# --- 1. Prepare the label column ---\n",
    "# Ensure label is integer type (0/1)\n",
    "df_model_ready = df_model_ready.withColumn(\n",
    "    \"StoreClosedLabel\", \n",
    "    col(\"StoreClosedLabel\").cast(\"integer\")\n",
    ")\n",
    "\n",
    "# --- 2. Split data into training and test sets ---\n",
    "train_data, test_data = df_model_ready.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training count: {train_data.count()}\")\n",
    "print(f\"Test count: {test_data.count()}\")\n",
    "\n",
    "# --- 3. Configure and train Random Forest ---\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    numTrees=100,                # Increase for better performance (tradeoff: longer training)\n",
    "    maxDepth=10,                 # Tune this parameter\n",
    "    minInstancesPerNode=5,        # Helps prevent overfitting\n",
    "    seed=42,\n",
    "    subsamplingRate=0.8,          # Use 80% of data for each tree\n",
    "    featureSubsetStrategy=\"sqrt\"  # Use sqrt(features) for each split\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# --- 4. Make predictions ---\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# --- 5. Evaluate model performance ---\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"\\n\\n=== MODEL PERFORMANCE ===\")\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "\n",
    "# --- 6. Inspect feature importance ---\n",
    "import pandas as pd\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": rf_model.featureImportances.toArray()\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE ===\")\n",
    "print(feature_importance)\n",
    "\n",
    "# --- 7. Save model (optional) ---\n",
    "# rf_model.save(\"dbfs:/path/to/rf_store_closures_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4f534b-32fb-4f6a-bc4f-72d524d54e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Investigate Data Leakage (Critical First Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eafc556-b0ac-4481-94b6-049a4cad9a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check correlation between features and target\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "for feature in feature_cols:\n",
    "    correlation = df_model_ready.select(corr(feature, \"StoreClosedLabel\")).first()[0]\n",
    "    print(f\"{feature}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a06e925-685f-44eb-95e7-a11dac2e723d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Fix Temporal Feature Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955dd7d9-348e-49de-b25e-c6d2ffec2155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, sum as spark_sum, col, datediff, current_date\n",
    "\n",
    "# Create proper window specification (assuming 'date' column exists)\n",
    "window_spec = Window.partitionBy().orderBy(\"date\")\n",
    "\n",
    "# Calculate correct lag features\n",
    "df_fixed = (df_model_ready\n",
    "    .withColumn(\"correct_prev_day_closed\", \n",
    "                lag(\"StoreClosedLabel\", 1).over(window_spec))\n",
    "    \n",
    "    .withColumn(\"rolling_3day_closures\", \n",
    "                spark_sum(col(\"StoreClosedLabel\")).over(window_spec.rowsBetween(-3, -1)))\n",
    "    \n",
    "    .withColumn(\"correct_3day_closure_rate\",\n",
    "                col(\"rolling_3day_closures\") / 3)\n",
    ")\n",
    "\n",
    "# Validate calculation\n",
    "display(df_fixed.select(\"date\", \"StoreClosedLabel\", \"correct_prev_day_closed\", \"correct_3day_closure_rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a950410-3bdc-44aa-88ed-73a3281b7d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remove Leaky Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b5e4a4-5f79-4a6f-b989-92c3f14c823a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "safe_feature_cols = [\n",
    "    \"IsWeekend\",\n",
    "    \"IsHoliday\",\n",
    "    \"month_num\",\n",
    "    \"is_rain\",\n",
    "    \"is_snow\",\n",
    "    \"is_clear\",\n",
    "    \"is_cloudy\",\n",
    "    \"WeekDayName_index\",\n",
    "    \"MonthName_index\"\n",
    "]\n",
    "\n",
    "# Update assembler\n",
    "safe_assembler = VectorAssembler(\n",
    "    inputCols=safe_feature_cols,\n",
    "    outputCol=\"safe_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_safe = safe_assembler.transform(df_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55125e86-1094-4028-aab8-a14a78af7c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Rebuild Model with Safe Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8252ae0-8c8c-4924-8b2b-949cc10d1054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_safe, test_safe = df_safe.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train new model\n",
    "rf_safe = RandomForestClassifier(\n",
    "    featuresCol=\"safe_features\",\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "safe_model = rf_safe.fit(train_safe)\n",
    "\n",
    "# Evaluate\n",
    "safe_predictions = safe_model.transform(test_safe)\n",
    "evaluator.evaluate(safe_predictions)  # Expect <1.0 AUC now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a4cd9b-a996-4fce-9cd4-af9c1be559fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Validate with Business Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a25692d-db02-4601-8a75-bd2d6e24e238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Generate predictions for the entire dataset\n",
    "full_predictions = safe_model.transform(df_safe)\n",
    "\n",
    "# 2. Analyze closure reasons\n",
    "closure_reasons = full_predictions.filter(col(\"StoreClosedLabel\") == 1).select(\n",
    "    \"date\", \n",
    "    \"IsWeekend\", \n",
    "    \"IsHoliday\", \n",
    "    \"is_snow\",\n",
    "    \"is_rain\",\n",
    "    \"prediction\",  # Now available after transformation\n",
    "    \"probability\"  # Useful for confidence analysis\n",
    ").distinct()\n",
    "\n",
    "display(closure_reasons.orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c43bea-6374-4536-92ec-8037a16da4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "# Get feature importance\n",
    "safe_importance = pd.DataFrame({\n",
    "    \"Feature\": safe_feature_cols,\n",
    "    \"Importance\": safe_model.featureImportances.toArray()\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"SAFE FEATURE IMPORTANCE:\")\n",
    "print(safe_importance)\n",
    "\n",
    "# Plot importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(safe_importance['Feature'], safe_importance['Importance'])\n",
    "plt.gca().invert_yaxis()  # Most important on top\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Safe Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f0af8bb-c814-43a6-9d3d-9f44867dbe6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weather Impact Analysis\n",
    "# Weather impact on closures\n",
    "weather_effect = full_predictions.groupBy(\n",
    "    \"is_rain\", \"is_snow\", \"is_clear\", \"is_cloudy\", \"is_other_weather\"\n",
    ").agg(\n",
    "    spark_sum(\"StoreClosedLabel\").alias(\"actual_closures\"),\n",
    "    spark_sum(\"prediction\").alias(\"predicted_closures\"),\n",
    "    (spark_sum(\"prediction\") / count(\"*\")).alias(\"closure_rate\")\n",
    ")\n",
    "\n",
    "display(weather_effect.orderBy(\"closure_rate\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbcda92-0add-450d-b03b-e8384df6c6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporal Patterns Analysis\n",
    "# Day of week patterns\n",
    "weekday_effect = full_predictions.groupBy(\"WeekDayName_index\").agg(\n",
    "    (spark_sum(\"StoreClosedLabel\") / count(\"*\")).alias(\"actual_closure_rate\"),\n",
    "    (spark_sum(\"prediction\") / count(\"*\")).alias(\"predicted_closure_rate\")\n",
    ")\n",
    "\n",
    "# Month patterns\n",
    "month_effect = full_predictions.groupBy(\"MonthName_index\").agg(\n",
    "    (spark_sum(\"StoreClosedLabel\") / count(\"*\")).alias(\"actual_closure_rate\"),\n",
    "    (spark_sum(\"prediction\") / count(\"*\")).alias(\"predicted_closure_rate\")\n",
    ")\n",
    "\n",
    "print(\"Weekday Closure Rates:\")\n",
    "display(weekday_effect.orderBy(\"WeekDayName_index\"))\n",
    "\n",
    "print(\"\\nMonthly Closure Rates:\")\n",
    "display(month_effect.orderBy(\"MonthName_index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac25676-2f5c-4e1e-95f9-13d475b6592b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc4ba02-fdde-455f-a5f6-f5fbb7c15971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create combined weather severity feature\n",
    "df_fixed = df_fixed.withColumn(\n",
    "    \"weather_severity\",\n",
    "    when(col(\"is_snow\") == 1, 3)\n",
    "    .when(col(\"is_rain\") == 1, 2)\n",
    "    .when(col(\"is_other_weather\") == 1, 2)\n",
    "    .otherwise(1)\n",
    ")\n",
    "\n",
    "# Add weekend+holiday combination\n",
    "df_fixed = df_fixed.withColumn(\n",
    "    \"special_day\",\n",
    "    when((col(\"IsWeekend\") == 1) | (col(\"IsHoliday\") == 1), 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Update feature list\n",
    "enhanced_features = safe_feature_cols + [\"weather_severity\", \"special_day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a99368-d789-4fa9-9c45-93130bc330ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Address Class Imbalance (if exist)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "class_balance = full_predictions.groupBy(\"StoreClosedLabel\").count()\n",
    "class_balance.show()\n",
    "\n",
    "# If imbalance > 10:1, use class weights\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    featuresCol=\"safe_features\",\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    weightCol=\"class_weight\"\n",
    ")\n",
    "\n",
    "# Create weight column\n",
    "df_weighted = df_safe.withColumn(\n",
    "    \"class_weight\",\n",
    "    when(col(\"StoreClosedLabel\") == 1, 10).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95f2536-b435-431b-9807-477b63cad0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(rf_safe.numTrees, [30, 50, 100])\n",
    "    .addGrid(rf_safe.maxDepth, [3, 5, 7])\n",
    "    .addGrid(rf_safe.minInstancesPerNode, [1, 5, 10])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=rf_safe,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_safe)\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928d858a-9c8d-4f8a-a2b7-8787cfa63701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Threshold Tuning (for precision/recall tradeoff):\n",
    "# Generate probability scores\n",
    "probabilities = best_model.transform(test_safe).select(\n",
    "    \"probability\", \"StoreClosedLabel\"\n",
    ").rdd.map(lambda row: (\n",
    "    float(row.probability[1]), \n",
    "    float(row.StoreClosedLabel)\n",
    ")).toDF([\"probability\", \"label\"])\n",
    "\n",
    "# Find optimal threshold\n",
    "thresholds = [i/100 for i in range(10, 60)]\n",
    "results = []\n",
    "for t in thresholds:\n",
    "    predicted = probabilities.withColumn(\"prediction\", when(col(\"probability\") > t, 1).otherwise(0))\n",
    "    precision = predicted.filter(col(\"prediction\") == 1).filter(col(\"label\") == 1).count() / max(1, predicted.filter(col(\"prediction\") == 1).count())\n",
    "    recall = predicted.filter(col(\"prediction\") == 1).filter(col(\"label\") == 1).count() / max(1, predicted.filter(col(\"label\") == 1).count())\n",
    "    results.append((t, precision, recall))\n",
    "\n",
    "# Plot precision-recall curve\n",
    "pd_results = pd.DataFrame(results, columns=[\"threshold\", \"precision\", \"recall\"])\n",
    "pd_results.plot(x=\"recall\", y=\"precision\", title=\"Precision-Recall Tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a007ebf0-bd48-4a43-b7f3-56612bcf42ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on your precision-recall tradeoff curve, here's a detailed analysis and recommendations for next steps:\n",
    "\n",
    "Precision-Recall Analysis\n",
    "Your curve shows a strong tradeoff between precision and recall, which is expected. Key observations:\n",
    "\n",
    "High precision (0.90) comes at the cost of low recall (0.65):\n",
    "\n",
    "You'll rarely predict closures incorrectly (good for avoiding false alarms)\n",
    "\n",
    "But you'll miss 35% of actual closures (risking unpreparedness)\n",
    "\n",
    "Balanced point (0.80 precision, 0.80 recall):\n",
    "\n",
    "Good compromise for most business scenarios\n",
    "\n",
    "Captures 80% of closures while keeping false alarms at 20%\n",
    "\n",
    "High recall (0.95+) comes at significant precision cost:\n",
    "\n",
    "Catches nearly all closures but with many false alarms\n",
    "\n",
    "Only suitable if missing closures is extremely costly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720c591c-f61a-45ef-91ff-5bbef1e7e7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply your chosen threshold to predictions\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# 1. Get probability of positive class (closure)\n",
    "get_positive_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "predictions = safe_predictions.withColumn(\"closure_prob\", get_positive_prob(\"probability\"))\n",
    "\n",
    "# 2. Apply optimal threshold\n",
    "CHOSEN_THRESHOLD = 0.40  # Balanced scenario\n",
    "final_predictions = predictions.withColumn(\n",
    "    \"optimized_prediction\",\n",
    "    when(col(\"closure_prob\") > CHOSEN_THRESHOLD, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Evaluate new predictions\n",
    "tp = final_predictions.filter((col(\"optimized_prediction\") == 1) & (col(\"StoreClosedLabel\") == 1)).count()\n",
    "fp = final_predictions.filter((col(\"optimized_prediction\") == 1) & (col(\"StoreClosedLabel\") == 0)).count()\n",
    "fn = final_predictions.filter((col(\"optimized_prediction\") == 0) & (col(\"StoreClosedLabel\") == 1)).count()\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Optimized Precision: {precision:.2f}\")\n",
    "print(f\"Optimized Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199a35d6-0df6-48c8-80ac-a22dd973739a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## FE\n",
    "# Create time-sensitive features\n",
    "from pyspark.sql.functions import dayofmonth, weekofyear\n",
    "\n",
    "df_fixed = (df_fixed\n",
    "    .withColumn(\"day_of_month\", dayofmonth(\"date\"))\n",
    "    .withColumn(\"week_of_year\", weekofyear(\"date\"))\n",
    "    .withColumn(\"is_month_end\", (dayofmonth(\"date\") >= 25).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Add weather severity index\n",
    "df_fixed = df_fixed.withColumn(\n",
    "    \"weather_severity\",\n",
    "    when(col(\"is_snow\") == 1, 3)\n",
    "    .when(col(\"is_rain\") == 1, 2)\n",
    "    .when(col(\"is_other_weather\") == 1, 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Update feature set\n",
    "enhanced_features = safe_feature_cols + [\n",
    "    \"day_of_month\", \n",
    "    \"week_of_year\",\n",
    "    \"is_month_end\",\n",
    "    \"weather_severity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b098e5a7-5c42-41e0-8311-0bd307d56e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Address Class Imbalanc\n",
    "# Check current class distribution\n",
    "class_balance = df_fixed.groupBy(\"StoreClosedLabel\").count()\n",
    "class_balance.show()\n",
    "\n",
    "# Apply class weights\n",
    "weighted_rf = RandomForestClassifier(\n",
    "    featuresCol=\"safe_features\",\n",
    "    labelCol=\"StoreClosedLabel\",\n",
    "    weightCol=\"class_weights\"\n",
    ")\n",
    "\n",
    "# Create weight column\n",
    "df_weighted = df_fixed.withColumn(\n",
    "    \"class_weights\",\n",
    "    when(col(\"StoreClosedLabel\") == 1, 5.0).otherwise(1.0)  # Upweight closure events\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac675cc5-7332-458b-97ce-d2c0a2bd9a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameter Tunin\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "    .addGrid(rf_safe.numTrees, [30, 50, 100])\n",
    "    .addGrid(rf_safe.maxDepth, [3, 5, 7])\n",
    "    .addGrid(rf_safe.minInstancesPerNode, [1, 5, 10])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=rf_safe,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=BinaryClassificationEvaluator(\n",
    "        labelCol=\"StoreClosedLabel\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderPR\"  # Focus on precision-recall curve\n",
    "    ),\n",
    "    numFolds=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = crossval.fit(train_safe)\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2f59b4-9892-445c-9b80-8d26b2f3b736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Business Rule Integration\n",
    "# Create fallback rules based on domain knowledge\n",
    "final_predictions = final_predictions.withColumn(\n",
    "    \"final_decision\",\n",
    "    when((col(\"is_snow\") == 1) & (col(\"optimized_prediction\") == 0), 1)  # Always predict closure in snow\n",
    "    .when((col(\"IsHoliday\") == 1) & (col(\"month_num\").isin([11,12])), 1)  # Holiday season closures\n",
    "    .otherwise(col(\"optimized_prediction\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735c7052-172e-4b61-a5c7-ad0b38fb6c14",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752154067023}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1752154619538}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14f94f8-c3a9-4886-aa65-015cb559ceee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_predictions.write.mode(\"overwrite\").saveAsTable(\"Store_Closure_Pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9014c60c-13e1-4a60-ad5f-0b637e8700b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NOW"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6421997426008486,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "STORE_closure2x",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}