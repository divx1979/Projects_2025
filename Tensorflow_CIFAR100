{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:45:57.977824Z","iopub.execute_input":"2025-11-20T13:45:57.978553Z","iopub.status.idle":"2025-11-20T13:45:57.982822Z","shell.execute_reply.started":"2025-11-20T13:45:57.978530Z","shell.execute_reply":"2025-11-20T13:45:57.982105Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# CIFAR\n\n**Project Overview**\n\nGoal:\n\nBuild a Custom CNN (no transfer learning) that can classify CIFAR-100 images into 100 classes with ~65% accuracy.\n\nDataset:\n\n\t•\tCIFAR-100: 50k train + 10k test, each image is 32×32 RGB, 100 classes.\n\t•\tExample classes: apple, chair, shark, pickup truck, leopard, etc.\n\nApproach:\n\t\n    1.\tLoad dataset & prepare data pipeline\n\t2.\tCreate augmentations\n\t3.\tBuild CNN with residual blocks, BatchNorm, Dropout\n\t4.\tTrain with smart learning rate schedule\n\t5.\tEvaluate performance\n**","metadata":{}},{"cell_type":"code","source":"# imports\n\nimport tensorflow as tf\nimport numpy as np\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:03.266727Z","iopub.execute_input":"2025-11-20T13:46:03.267002Z","iopub.status.idle":"2025-11-20T13:46:03.270976Z","shell.execute_reply.started":"2025-11-20T13:46:03.266980Z","shell.execute_reply":"2025-11-20T13:46:03.270192Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from tensorflow.keras import layers, models, regularizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:04.387828Z","iopub.execute_input":"2025-11-20T13:46:04.388508Z","iopub.status.idle":"2025-11-20T13:46:04.392264Z","shell.execute_reply.started":"2025-11-20T13:46:04.388484Z","shell.execute_reply":"2025-11-20T13:46:04.391461Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"What’s happening:\n\n\t•\ttensorflow → our deep learning framework\n\t•\tlayers, models, regularizers → Keras tools for building CNNs\n    \n\t•\tnumpy → for array math\n\t•\tmath → for cosine learning rate scheduling","metadata":{}},{"cell_type":"markdown","source":"# PART 3 — Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"Concept:\n\nThink of these like dials on a control board:\n\n\t•\tBATCH_SIZE: number of images per step\n\t•\tEPOCHS: how many passes over the training data\n\t•\tMIXUP_ALPHA: controls MixUp augmentation (image blending)\n\t•\tLABEL_SMOOTHING: prevents overconfidence in predictions\n\t•\tWEIGHT_DECAY: small penalty on weights to avoid                   overfitting\n    \n\t•\tLR (Learning Rate): how fast the model learns\n\t•\tMOMENTUM: helps SGD optimizer “remember” direction\n\t•\tWARMUP: slowly increase LR early to stabilize training","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\nIMG_SHAPE = (32, 32, 3)\nNUM_CLASSES = 100\nEPOCHS = 200\n\nMIXUP_ALPHA = 0.2\nLABEL_SMOOTHING = 0.1\nWEIGHT_DECAY = 1e-4\n\nINITIAL_LR = 0.1\nMOMENTUM = 0.9\nWARMUP_EPOCHS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:49.169975Z","iopub.execute_input":"2025-11-20T13:46:49.170331Z","iopub.status.idle":"2025-11-20T13:46:49.174489Z","shell.execute_reply.started":"2025-11-20T13:46:49.170308Z","shell.execute_reply":"2025-11-20T13:46:49.173751Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# PART 4 — Data Loading\n\nConcept:\n\n\t•\tTensorFlow gives you CIFAR-100 ready to go.\n\t•\tEach x = images, y = class labels.\n\t•\tlabel_mode='fine' → 100 detailed classes (not grouped into 20 superclasses).\n\t•\ttf.squeeze removes unnecessary shape dimensions","metadata":{}},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode = 'fine')\ny_train = tf.squeeze(y_train)\ny_test = tf.squeeze(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:54.455077Z","iopub.execute_input":"2025-11-20T13:46:54.455601Z","iopub.status.idle":"2025-11-20T13:46:56.994627Z","shell.execute_reply.started":"2025-11-20T13:46:54.455577Z","shell.execute_reply":"2025-11-20T13:46:56.993971Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# PART 5 — Data Preprocessing (Train vs Test)\n\nConcept:\n\n\t•\tNormalization: divide by 255 to get pixels between [0,1]\n\t•\tAugmentation (train only): random crop, flip, brightness — makes model more robust\n\t•\tNo augmentation on test data, only normalize","metadata":{}},{"cell_type":"code","source":" # Training Prep\ndef preprocess_train(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize_with_crop_or_pad(image, 36, 36)\n    image = tf.image.random_crop(image, [32, 32, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, 0.1)\n    image = tf.image.random_contrast(image, 0.9, 1.1)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:03.135652Z","iopub.execute_input":"2025-11-20T13:47:03.136174Z","iopub.status.idle":"2025-11-20T13:47:03.141059Z","shell.execute_reply.started":"2025-11-20T13:47:03.136141Z","shell.execute_reply":"2025-11-20T13:47:03.140104Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Eval Prep\ndef preprocess_eval(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:04.245355Z","iopub.execute_input":"2025-11-20T13:47:04.245645Z","iopub.status.idle":"2025-11-20T13:47:04.249752Z","shell.execute_reply.started":"2025-11-20T13:47:04.245624Z","shell.execute_reply":"2025-11-20T13:47:04.249001Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# PART 6 — Build the TensorFlow Dataset Pipeline\n\nConcept:\n\n\t•\ttf.data.Dataset efficiently feeds data to GPU.\n\t•\tshuffle prevents the model from memorizing the order.\n\t•\tmap applies your preprocessing function.\n\t•\tprefetch overlaps data loading with GPU work for speed","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.shuffle(50000)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls = tf.data.AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\ntest_ds = test_ds.map(preprocess_eval).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:07.489593Z","iopub.execute_input":"2025-11-20T13:47:07.489929Z","iopub.status.idle":"2025-11-20T13:47:08.474799Z","shell.execute_reply.started":"2025-11-20T13:47:07.489892Z","shell.execute_reply":"2025-11-20T13:47:08.474185Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# PART 7 — CNN Building Blocks\n\nConcept:\n\n\t•\tConv2D → extracts features (edges, textures, etc.)\n\t•\tBatchNorm → stabilizes and speeds up learning\n\t•\tReLU → adds non-linearity","metadata":{}},{"cell_type":"code","source":"def conv_bn_relu(x, filters, kernel_size=3, stride=1):\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:12.415576Z","iopub.execute_input":"2025-11-20T13:47:12.416089Z","iopub.status.idle":"2025-11-20T13:47:12.420087Z","shell.execute_reply.started":"2025-11-20T13:47:12.416067Z","shell.execute_reply":"2025-11-20T13:47:12.419500Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# PART 8 — Residual Block\n\nConcept:\n\nResidual blocks = ResNet magic\n\nThey skip connections, letting gradients flow easily through deep networks.\n\nYou can think: “learn the change (residual) instead of starting from scratch each layer\"","metadata":{}},{"cell_type":"code","source":"def residual_block(x, filters, stride=1):\n    shortcut = x\n    out = conv_bn_relu(x, filters, 3, stride)\n    out = layers.Conv2D(filters, 3, padding='same', use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n\n    # adjust shortcut size if needed\n    if stride != 1 or x.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.add([out, shortcut])\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:15.032348Z","iopub.execute_input":"2025-11-20T13:47:15.032646Z","iopub.status.idle":"2025-11-20T13:47:15.037753Z","shell.execute_reply.started":"2025-11-20T13:47:15.032624Z","shell.execute_reply":"2025-11-20T13:47:15.036999Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# PART 10 — Full CNN Architecture\n\nConcept:\n\n\t•\tDeep stacked residual layers → feature extraction\n\t•\tGlobalAveragePooling + GlobalMaxPooling → captures both mean and extreme features\n\t•\tDense layer → final classifier\n\t•\tSoftmax → converts to probabilities","metadata":{}},{"cell_type":"code","source":"def build_custom_cnn(input_shape=(32,32,3), num_classes=100):\n    inputs = layers.Input(shape=input_shape)\n    x = conv_bn_relu(inputs, 64)\n    x = residual_block(x, 64)\n    x = residual_block(x, 128, stride=2)\n    x = residual_block(x, 256, stride=2)\n    x = residual_block(x, 512, stride=2)\n    x = se_block(x)  # optional\n\n    # Advanced pooling: average + max\n    gap = layers.GlobalAveragePooling2D()(x)\n    gmp = layers.GlobalMaxPooling2D()(x)\n    x = layers.Concatenate()([gap, gmp])\n\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    return models.Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:47:20.851540Z","iopub.execute_input":"2025-11-20T13:47:20.852324Z","iopub.status.idle":"2025-11-20T13:47:20.857800Z","shell.execute_reply.started":"2025-11-20T13:47:20.852299Z","shell.execute_reply":"2025-11-20T13:47:20.856982Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# PART 11 — Learning Rate Schedule (Cosine + Warmup\n\nConcept\n:\n\t•\tStarts small (warmup) → prevents exploding gradients early\n\t•\tSlowly decays following a cosine wave — helps convergence\n\t•\tThink of it as “start slow, go fast, then cool down smoothl","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport math\n\nclass WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, base_lr, epochs, steps_per_epoch, warmup_epochs=5, name=None):\n        super().__init__()\n        self.base_lr = tf.convert_to_tensor(base_lr, dtype=tf.float32)\n        self.epochs = int(epochs)\n        self.steps_per_epoch = int(steps_per_epoch)\n        self.warmup_steps = tf.cast(warmup_epochs * self.steps_per_epoch, tf.float32)\n        self.total_steps = tf.cast(self.epochs * self.steps_per_epoch, tf.float32)\n        self.name = name or \"WarmUpCosine\"\n\n    def __call__(self, step):\n        # make sure step is float32 tensor\n        step = tf.cast(step, tf.float32)\n\n        # Warmup LR: linear ramp from 0 -> base_lr over warmup_steps\n        # Avoid division by zero if warmup_steps == 0\n        warmup_steps = tf.maximum(self.warmup_steps, 1.0)\n        warmup_lr = self.base_lr * (step / warmup_steps)\n\n        # Cosine decay part (after warmup)\n        progress = (step - self.warmup_steps) / tf.maximum(1.0, (self.total_steps - self.warmup_steps))\n        # clip progress to [0,1]\n        progress = tf.clip_by_value(progress, 0.0, 1.0)\n        cosine_decay = 0.5 * (1.0 + tf.cos(math.pi * progress))\n        cosine_lr = self.base_lr * cosine_decay\n\n        # If step < warmup_steps -> warmup_lr else cosine_lr\n        lr = tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)\n        # if step > total_steps, keep lr at 0 (optional) — here we keep clipped cosine_lr (already clipped)\n        return lr\n\n    def get_config(self):\n        return {\n            \"base_lr\": float(self.base_lr.numpy()) if tf.executing_eagerly() else float(self.base_lr),\n            \"epochs\": self.epochs,\n            \"steps_per_epoch\": self.steps_per_epoch,\n            \"warmup_epochs\": int(self.warmup_steps.numpy() // self.steps_per_epoch) if tf.executing_eagerly() else int(self.warmup_steps / self.steps_per_epoch),\n            \"name\": self.name\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:01:48.001153Z","iopub.execute_input":"2025-11-20T14:01:48.001564Z","iopub.status.idle":"2025-11-20T14:01:48.010646Z","shell.execute_reply.started":"2025-11-20T14:01:48.001539Z","shell.execute_reply":"2025-11-20T14:01:48.009785Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"steps_per_epoch = math.ceil(len(x_train) / BATCH_SIZE)\nlr_schedule = WarmUpCosine(INITIAL_LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, warmup_epochs=WARMUP_EPOCHS)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\n\n# If you use categorical one-hot labels:\n# loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n# otherwise for integer labels:\n# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:11.277612Z","iopub.execute_input":"2025-11-20T14:02:11.278422Z","iopub.status.idle":"2025-11-20T14:02:11.287951Z","shell.execute_reply.started":"2025-11-20T14:02:11.278395Z","shell.execute_reply":"2025-11-20T14:02:11.287192Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# PART 12 — Compile the Mode\n\nConcept\n:\n\t•\tSGD + Momentum → stable, proven optimizer for CNNs\n\t•\tLabel smoothing → prevents model from becoming overconfident\n\t•\tCompile → tie the model, loss, and optimizer together before trainin","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar100\n\n(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n\ny_train = y_train.squeeze()\ny_test = y_test.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:23.846343Z","iopub.execute_input":"2025-11-20T14:02:23.846921Z","iopub.status.idle":"2025-11-20T14:02:26.377138Z","shell.execute_reply.started":"2025-11-20T14:02:23.846897Z","shell.execute_reply":"2025-11-20T14:02:26.376552Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:29.578174Z","iopub.execute_input":"2025-11-20T14:02:29.578822Z","iopub.status.idle":"2025-11-20T14:02:29.582459Z","shell.execute_reply.started":"2025-11-20T14:02:29.578795Z","shell.execute_reply":"2025-11-20T14:02:29.581609Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def se_block(x, se_ratio=8):\n    filters = x.shape[-1]\n    se = tf.keras.layers.GlobalAveragePooling2D()(x)\n    se = tf.keras.layers.Dense(filters // se_ratio, activation='relu')(se)\n    se = tf.keras.layers.Dense(filters, activation='sigmoid')(se)\n    se = tf.keras.layers.Reshape((1,1,filters))(se)\n    return tf.keras.layers.multiply([x, se])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:31.570213Z","iopub.execute_input":"2025-11-20T14:02:31.571124Z","iopub.status.idle":"2025-11-20T14:02:31.575512Z","shell.execute_reply.started":"2025-11-20T14:02:31.571094Z","shell.execute_reply":"2025-11-20T14:02:31.574814Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE\nlr_schedule = WarmUpCosine(INITIAL_LR, EPOCHS, steps_per_epoch)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n\nmodel = build_custom_cnn()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:33.855163Z","iopub.execute_input":"2025-11-20T14:02:33.855711Z","iopub.status.idle":"2025-11-20T14:02:34.278376Z","shell.execute_reply.started":"2025-11-20T14:02:33.855688Z","shell.execute_reply":"2025-11-20T14:02:34.277645Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"# PART 13 — Callback\n\nConcept:\n\t•\tCheckpoint: saves your best weights\n\t•\tReduceLROnPlateau: lowers LR if loss stops improving\n\t•\tEarlyStopping: stops training early to prevent overfittin","metadata":{}},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:02:40.543898Z","iopub.execute_input":"2025-11-20T14:02:40.544158Z","iopub.status.idle":"2025-11-20T14:02:40.548202Z","shell.execute_reply.started":"2025-11-20T14:02:40.544140Z","shell.execute_reply":"2025-11-20T14:02:40.547659Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Part 14: Train Model\n\nConcept:\n* Each epoch = full pass over train data\n* Validation used To Track Gen\n* Callback Help Tune Automatically While Training","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:03:13.295669Z","iopub.execute_input":"2025-11-20T14:03:13.295952Z","iopub.status.idle":"2025-11-20T14:03:13.300077Z","shell.execute_reply.started":"2025-11-20T14:03:13.295934Z","shell.execute_reply":"2025-11-20T14:03:13.299302Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Use original integer labels y_train, y_test (shape (N,)) — do NOT one-hot encode\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_ds = train_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)   # your augment fn must return (image, int_label)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_ds = test_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# Compile with sparse loss (no label smoothing available on older TF)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n# or metrics=['accuracy'] — Keras will infer correct accuracy for sparse targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:03:14.751507Z","iopub.execute_input":"2025-11-20T14:03:14.751811Z","iopub.status.idle":"2025-11-20T14:03:15.660638Z","shell.execute_reply.started":"2025-11-20T14:03:14.751790Z","shell.execute_reply":"2025-11-20T14:03:15.659731Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# 1) Create optimizer with a float LR\noptimizer = tf.keras.optimizers.SGD(learning_rate=INITIAL_LR, momentum=MOMENTUM, nesterov=True)\n\n# 2) Define an epoch-based LR function (returns lr for given epoch)\ndef epoch_warmup_cosine(epoch):\n    # simple epoch-based schedule: linear warmup -> cosine decay across EPOCHS\n    warmup = WARMUP_EPOCHS\n    if epoch < warmup:\n        return float(INITIAL_LR * (epoch + 1) / warmup)  # +1 so epoch0 != 0\n    # cosine decay after warmup\n    progress = (epoch - warmup) / max(1, (EPOCHS - warmup))\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return float(INITIAL_LR * cosine)\n\n# 3) Use LearningRateScheduler (Keras will set optimizer.lr)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(epoch_warmup_cosine, verbose=1)\n\n# 4) Use ReduceLROnPlateau as well (it will set lr when plateau occurs)\nreduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=6, verbose=1, mode='min'\n)\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max', verbose=1),\n    lr_callback,\n    reduce_on_plateau\n]\n\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:13:08.641337Z","iopub.execute_input":"2025-11-20T14:13:08.641789Z","iopub.status.idle":"2025-11-20T14:13:08.661011Z","shell.execute_reply.started":"2025-11-20T14:13:08.641766Z","shell.execute_reply":"2025-11-20T14:13:08.660358Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=test_ds,\n    epochs=EPOCHS,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T14:13:16.806234Z","iopub.execute_input":"2025-11-20T14:13:16.806553Z","iopub.status.idle":"2025-11-20T14:22:24.440271Z","shell.execute_reply.started":"2025-11-20T14:13:16.806533Z","shell.execute_reply":"2025-11-20T14:22:24.439632Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1: LearningRateScheduler setting learning rate to 0.02.\nEpoch 1/200\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.0088 - loss: 4.6074\nEpoch 1: val_accuracy improved from -inf to 0.01610, saving model to best_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 87ms/step - accuracy: 0.0088 - loss: 4.6074 - val_accuracy: 0.0161 - val_loss: 4.6776 - learning_rate: 0.0200\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.04.\nEpoch 2/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0086 - loss: 4.6066\nEpoch 2: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0086 - loss: 4.6066 - val_accuracy: 0.0155 - val_loss: 4.7323 - learning_rate: 0.0400\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.06000000000000001.\nEpoch 3/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0092 - loss: 4.6068\nEpoch 3: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 66ms/step - accuracy: 0.0092 - loss: 4.6068 - val_accuracy: 0.0126 - val_loss: 4.6082 - learning_rate: 0.0600\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.08.\nEpoch 4/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0095 - loss: 4.6073\nEpoch 4: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 66ms/step - accuracy: 0.0095 - loss: 4.6073 - val_accuracy: 0.0111 - val_loss: 4.6186 - learning_rate: 0.0800\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.1.\nEpoch 5/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0089 - loss: 4.6078\nEpoch 5: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0089 - loss: 4.6078 - val_accuracy: 0.0129 - val_loss: 4.6090 - learning_rate: 0.1000\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.1.\nEpoch 6/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 6: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0085 - val_loss: 4.6180 - learning_rate: 0.1000\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.09999351124856874.\nEpoch 7/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 7: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0088 - val_loss: 4.6122 - learning_rate: 0.1000\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.09997404667843075.\nEpoch 8/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 8: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0093 - val_loss: 4.6073 - learning_rate: 0.1000\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.09994161134161633.\nEpoch 9/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0083 - loss: 4.6080\nEpoch 9: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0083 - loss: 4.6080 - val_accuracy: 0.0113 - val_loss: 4.6100 - learning_rate: 0.0999\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.09989621365671902.\nEpoch 10/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 10: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0094 - val_loss: 4.6071 - learning_rate: 0.0999\n\nEpoch 11: LearningRateScheduler setting learning rate to 0.09983786540671051.\nEpoch 11/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 11: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0107 - val_loss: 4.6066 - learning_rate: 0.0998\n\nEpoch 12: LearningRateScheduler setting learning rate to 0.09976658173588243.\nEpoch 12/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 12: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0101 - val_loss: 4.6069 - learning_rate: 0.0998\n\nEpoch 13: LearningRateScheduler setting learning rate to 0.09968238114591566.\nEpoch 13/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 13: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0100 - val_loss: 4.6065 - learning_rate: 0.0997\n\nEpoch 14: LearningRateScheduler setting learning rate to 0.0995852854910781.\nEpoch 14/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0087 - loss: 4.6080\nEpoch 14: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0087 - loss: 4.6080 - val_accuracy: 0.0101 - val_loss: 4.6064 - learning_rate: 0.0996\n\nEpoch 15: LearningRateScheduler setting learning rate to 0.09947531997255256.\nEpoch 15/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 15: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0105 - val_loss: 4.6063 - learning_rate: 0.0995\n\nEpoch 16: LearningRateScheduler setting learning rate to 0.09935251313189564.\nEpoch 16/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 16: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0105 - val_loss: 4.6064 - learning_rate: 0.0994\n\nEpoch 17: LearningRateScheduler setting learning rate to 0.09921689684362989.\nEpoch 17/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 17: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0102 - val_loss: 4.6064 - learning_rate: 0.0992\n\nEpoch 18: LearningRateScheduler setting learning rate to 0.09906850630697067.\nEpoch 18/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 18: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0102 - val_loss: 4.6064 - learning_rate: 0.0991\n\nEpoch 19: LearningRateScheduler setting learning rate to 0.09890738003669029.\nEpoch 19/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 19: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0107 - val_loss: 4.6064 - learning_rate: 0.0989\n\nEpoch 20: LearningRateScheduler setting learning rate to 0.0987335598531214.\nEpoch 20/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 20: val_accuracy did not improve from 0.01610\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0101 - val_loss: 4.6064 - learning_rate: 0.0987\n\nEpoch 21: LearningRateScheduler setting learning rate to 0.0985470908713026.\nEpoch 21/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 21: val_accuracy did not improve from 0.01610\n\nEpoch 21: ReduceLROnPlateau reducing learning rate to 0.049273546785116196.\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0103 - val_loss: 4.6064 - learning_rate: 0.0985\nEpoch 21: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}