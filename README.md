# Projects_2025
All My Projects In 2025

Projects In The Order:
1. Store Closure

   # Store Closure Prediction Across Multi-Store Retail

A scalable ML pipeline (PySpark, Databricks-ready) to forecast store closures using sales, weather, and calendar data, generalized for multiple retail entities.

---

## Features
- End-to-end pipeline: Data prep â†’ Feature engineering â†’ Leakage-safe training â†’ Evaluation
- Synthetic/demo data for reproducibility
- Proactive closure alerts for operational excellence

---

## How to Run

1. Clone repo: `git clone ...`
2. (Optional) Create/activate a virtual environment
3. Install dependencies: `pip install -r requirements.txt`
4. Open notebooks or run scripts in `src/`

---

## Folders

- `data/`: Demo synthetic CSVs
- `notebooks/`: Jupyter Notebooks (PySpark code)
- `src/`: Modular Python scripts for production

---

## Quickstart (Jupyter/Databricks)

```python
# 1. Generate or load synthetic data
# 2. Prepare and join dataframes
# 3. Feature engineering (rolling stats, lags, weather categoricals)
# 4. Split train/test (to avoid leakage)
# 5. Train ML model (RandomForestClassifier)
# 6. Evaluate & interpret results
exit
```

Project 2: Daily Prod Sales Affected By Weather Data
# ğŸ›’ Multi-Store Sales Forecasting (Weather & Holiday-Aware)

This project demonstrates an end-to-end pipeline to forecast **daily product sales** across multiple store locations using PySpark. It incorporates features like holidays, weekends, and seasonal trends to improve prediction accuracy, even in the absence of real-world data.

## ğŸ“Œ Objective
Predict daily sales for each store across the U.S. using a global model that generalizes across:
- Locations (businessEntityId)
- Product categories
- Time periods (holidays, weekends, seasons)

---

## ğŸ”§ Tools & Technologies

- **Apache Spark (PySpark)**: Scalable distributed processing
- **Databricks / Spark Notebooks**: Development and data exploration
- **MLlib**: Machine Learning with Random Forest, XGBoost
- **Pandas** & **matplotlib/seaborn**: Local testing and visualization
- **Scikit-learn GridSearchCV (conceptual)**: Hyperparameter tuning
- **Git & GitHub**: Version control

---

## ğŸ“Š Features Engineered

- **Temporal**: Day of Week, Month, IsHoliday, IsWeekend, Rolling Averages
- **Lag Features**: 1-day, 7-day lags of sales
- **Store-Level**: businessEntityId (store ID), Product ID, etc.
- **Weather-aware model** (optional): Precipitation, Temperature, Humidity, etc.

---

## ğŸš€ ML Pipeline Steps

1. **Mock Data Generation** (for offline testing)
2. **Feature Engineering**
3. **Model Building** using RandomForestRegressor
4. **Evaluation** using RMSE, MAE, and MAPE
5. **Hyperparameter Tuning** with Grid Search (planned extension)
6. **Deployment-Ready Pipeline Structure**

---

## ğŸ§ª Model Performance (Sample Results)

| Metric | Value |
|--------|-------|
| RMSE   | 105.31 |
| MAE    | 27.47 |
| MAPE   | 103.00% |

*MAPE can be optimized further with model tuning and refined seasonal adjustments.*

---

## ğŸ“ Structure

Project 3: Sentiment Categorization w/ NLP

Sentiment Categorization & Retail Category Mapping

Table of Contents
	1.	Project Overview
	2.	Architecture
	3.	Prerequisites
	4.	Setup & Installation
	5.	Data Preparation (Bronze)
	6.	Feature Engineering (Silver)
	7.	Model Training & Registration
	8.	Inference & Gold Table
	9.	MLOps Pipeline (CI/CD)
	10.	Directory Structure
	11.	Next Steps

â¸»

Project Overview

This end-to-end project ingests social media comments, performs sentiment and semantic categorization using SBERT and GPT-3.5, enriches with metadata (store weather, departments), trains a classification model, and deploys it on Azure Databricks with MLflow and a CI/CD pipeline.

Architecture

Raw Data (unified_posts_comments_testing)
        â†“ Bronze Cleaning & Deduplication
ml_bronze.comments_clean â†’ pandas â†’ text embeddings & HTTP calls
        â†“ Silver Feature Engineering & Enrichment
ml_silver.comments_features â†’ model training (MLflow) â†’ registered model
        â†“ Inference Pipeline
gold_dev.sentiment_predictions â†’ downstream reporting

Additional GPT-based mapping for retail categories and store departments writes to gold_dev.sentiment_classify.

Prerequisites
	â€¢	Azure Databricks workspace with attached cluster
	â€¢	Access to OpenAI / Azure OpenAI (API key & deployment)
	â€¢	MLflow workspace or Databricks-managed MLflow
	â€¢	Tables: gold_dev.unified_posts_comments_testing, bronze.ovvi_department
	â€¢	Databricks CLI or Git integration for CI/CD

Setup & Installation
	1.	Clone the repository into Databricks Repos.
	2.	Install required libraries on your cluster:

%pip install sentence-transformers requests mlflow


	3.	Configure environment variables or notebook-scoped variables for API keys and endpoints.

Data Preparation (Bronze)
	â€¢	Notebook: 01_data_prep
	â€¢	Reads gold_dev.unified_posts_comments_testing, cleans nulls, drops duplicates on comment_id and comment_text.
	â€¢	Writes cleaned table to ml_bronze.comments_clean.

Feature Engineering (Silver)
	â€¢	Notebook: 02_feature_engineering
	â€¢	SBERT semantic categorization with all-MiniLM-L6-v2.
	â€¢	GPT-3.5 mapping for retail categories and departments via REST API.
	â€¢	Persists enriched table to ml_silver.comments_features.

Model Training & Registration
	â€¢	Notebook: 03_train_register
	â€¢	Loads silver table, extracts embeddings, trains a logistic regression classifier.
	â€¢	Logs experiment and metrics to MLflow.
	â€¢	Registers the model as SentimentCatModel with versioning.

Inference & Gold Table
	â€¢	Notebook: 04_inference
	â€¢	Loads registered model from MLflow model registry.
	â€¢	Scores new comments, writes predictions to gold_dev.sentiment_predictions (or gold_dev.sentiment_classify).

MLOps Pipeline (CI/CD)
	â€¢	Azure DevOps (or GitHub Actions) YAML pipelines to:
	1.	Build & Test: import notebooks, run 01_data_prep and 02_feature_engineering.
	2.	Train & Register: execute 03_train_register.
	3.	Deploy: configure a scheduled job to execute 04_inference daily.
	â€¢	Example pipeline files in ci/ directory (config_test_run.json, config_train_run.json, config_inference_job.json).

Directory Structure

/Repos/YourProject
â”œâ”€â”€ 01_data_prep.ipynb
â”œâ”€â”€ 02_feature_engineering.ipynb
â”œâ”€â”€ 03_train_register.ipynb
â”œâ”€â”€ 04_inference.ipynb
â”œâ”€â”€ ci/
â”‚   â”œâ”€â”€ azure-pipelines.yml
â”‚   â”œâ”€â”€ config_test_run.json
â”‚   â”œâ”€â”€ config_train_run.json
â”‚   â””â”€â”€ config_inference_job.json
â””â”€â”€ README.md

Next Steps
	â€¢	Implement monitoring on prediction drift and data quality.
	â€¢	Add automated retraining triggers based on performance metrics.
	â€¢	Extend the classification model to multi-label or hierarchical categories.
	â€¢	Integrate real-time streaming inference using Databricks Structured Streaming.

â¸»

Prepared by Data Engineering & ML Team
