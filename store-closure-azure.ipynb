{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Project Structure","metadata":{}},{"cell_type":"code","source":"# store-closure-prediction/\n# â”‚\n# â”œâ”€â”€ data/\n# â”‚   â”œâ”€â”€ synthetic_sales.csv\n# â”‚   â”œâ”€â”€ synthetic_weather.csv\n# â”‚   â””â”€â”€ synthetic_calendar.csv\n# â”‚\n# â”œâ”€â”€ notebooks/\n# â”‚   â”œâ”€â”€ 01_data_preparation.ipynb\n# â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb\n# â”‚   â”œâ”€â”€ 03_model_training.ipynb\n# â”‚   â””â”€â”€ 04_evaluation_reporting.ipynb\n# â”‚\n# â”œâ”€â”€ src/\n# â”‚   â”œâ”€â”€ data_utils.py\n# â”‚   â”œâ”€â”€ features.py\n# â”‚   â”œâ”€â”€ model.py\n# â”‚   â””â”€â”€ utils.py\n# â”‚\n# â”œâ”€â”€ README.md\n# â””â”€â”€ requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Store Closure Prediction Across Multi-Store Retail\n\nA scalable ML pipeline (PySpark, Databricks-ready) to forecast store closures using sales, weather, and calendar data, generalized for multiple retail entities.\n\n---\n\n## Features\n- End-to-end pipeline: Data prep â†’ Feature engineering â†’ Leakage-safe training â†’ Evaluation\n- Synthetic/demo data for reproducibility\n- Proactive closure alerts for operational excellence\n\n---\n\n## How to Run\n\n1. Clone repo: `git clone ...`\n2. (Optional) Create/activate a virtual environment\n3. Install dependencies: `pip install -r requirements.txt`\n4. Open notebooks or run scripts in `src/`\n\n---\n\n## Folders\n\n- `data/`: Demo synthetic CSVs\n- `notebooks/`: Jupyter Notebooks (PySpark code)\n- `src/`: Modular Python scripts for production\n\n---\n\n## Quickstart (Jupyter/Databricks)\n\n```python\n# 1. Generate or load synthetic data\n# 2. Prepare and join dataframes\n# 3. Feature engineering (rolling stats, lags, weather categoricals)\n# 4. Split train/test (to avoid leakage)\n# 5. Train ML model (RandomForestClassifier)\n# 6. Evaluate & interpret results","metadata":{}},{"cell_type":"markdown","source":"Tools Used\n\t\n    â€¢\tPySpark, Spark MLlib, Pandas\n\tâ€¢\tDatabricks/Spark cluster (local compatible)\n\tâ€¢\tPython 3.8+\n\nâ¸»\n\nBenefits\n\t\n    â€¢\tPrevents costly unplanned closures\n\tâ€¢\tData-driven planning for staffing, inventory, revenue protection\n\tâ€¢\tScalable to 100s of stores/entities","metadata":{}},{"cell_type":"code","source":"# ---\n\n# ## 3. ğŸ”§ Code/Notebook Steps (with Demo Data)\n\n# ### `01_data_preparation.ipynb`\n# - Generate synthetic data with Pandas (or supply demo CSVs)\n# - Load into PySpark DataFrames  \n# - Data schema: sales, weather, calendar, entity metadata\n\n# ### `02_feature_engineering.ipynb`\n# - Merge sales/weather/calendar by date/entity/SiteID\n# - Create closure label (1 = closed, 0 = open)\n# - Feature engineer: lags, rolling closure rate, weather type bins, calendar flags\n# - **Split train/test BEFORE feature engineering!** (to avoid leakage)\n\n# ### `03_model_training.ipynb`\n# - Assemble feature columns (only those available at prediction time)\n# - Train RandomForestClassifier or any ML model\n# - Track ROC, accuracy, feature importance\n\n# ### `04_evaluation_reporting.ipynb`\n# - Evaluate on holdout (test) data\n# - Show ROC/accuracy, confusion matrix\n# - Plot sample predictions, errors\n\n# ---\n\n# ## 4. ğŸ§ª `data/synthetic_sales.csv` Example\n\n# ```csv\n# date,businessEntityId,SiteID,TotalOrders\n# 2023-01-01,1,101,50\n# 2023-01-01,2,201,0\n# ...","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. ğŸ“¦ requirements.txt","metadata":{}},{"cell_type":"markdown","source":"pyspark\n\npandas\n\nscikit-learn\n\njupyter","metadata":{}},{"cell_type":"markdown","source":"6. ğŸ“¢ Commit Message Examples\n\t\n\n   â€¢\tInitial project skeleton and README\n\t\n\n   â€¢\tAdd synthetic data generators\n\t\n\n   â€¢\tAdd feature engineering and leakage-safe split\n\t\n\n   â€¢\tAdd model training and evaluation notebook","metadata":{}},{"cell_type":"markdown","source":"7. âœ¨ Make it Engaging!\n\t\n\n   â€¢\tAdd badges (build, license)\n\t\n\n   â€¢\tInclude results screenshots or ROC curves\n\t\n\n   â€¢\tWrite short explanations at top of each notebook\n\t\n\n   â€¢\tOffer a â€œTry this yourself!â€ cell in each notebook","metadata":{}},{"cell_type":"markdown","source":"Ready-to-paste demo code (for notebooks or src/ scripts)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pyspark.sql import SparkSession\n\n# 1. Synthetic sales data\nsales = pd.DataFrame({\n    'date': pd.date_range('2023-01-01', periods=90),\n    'businessEntityId': [1, 2]*45,\n    'SiteID': [101, 201]*45,\n    'TotalOrders': [50, 0]*45\n})\nsales.to_csv('data/synthetic_sales.csv', index=False)\n\n# 2. Load into Spark\nspark = SparkSession.builder.getOrCreate()\ndf_sales = spark.read.csv('data/synthetic_sales.csv', header=True, inferSchema=True)\ndf_sales.show(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}